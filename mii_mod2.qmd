---
title: Módulo 2
subtitle: Modelos de Regresión
format:
  clean-revealjs:
    self-contained: true
    theme: slides.scss
    touch: true
    slide-level: 2
author:
  - name: Eloy Alvarado Narváez
    orcid: 0000-0001-7522-2327
    email: eloy.alvarado@usm.cl
    affiliations: Universidad Técnica Federico Santa María
  - name: Esteban Salgado Valenzuela
    orcid: 0000-0002-7799-0044
    affiliations: Universidad Técnica Federico Santa María
date: 12/13/2024
lang: es
logo: images/logo_usm.png
bibliography: refs.bib
---

## Regresión Lineal

- Regresión lineal es un método simple para el aprendizaje supervisado.
- Asume que la dependencia de $Y$ en $X_1,\ldots,X_p$ es lineal
$$Y = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p$$
- Las funciones de regresión reales [nunca]{.alert} son lineales.
- Aunque simple, es útil teórica y prácticamente.

<center>
![](./images/mod2/fig0.png){width=50%}
</center>

# Regresión lineal simple{background-color="#40666e"}

## Regresión lineal simple
- Recordando los datos de publicidad:
  + ¿Hay relación entre el presupuesto de publicidad y las ventas?
  + ¿Qué tan fuerte es la relación?
  + ¿Qué medio contribuye a las ventas?
  + ¿Qué tan precisamente se pueden predecir ventas futuas?
  + ¿Es lineal la relación o hay *interacciones*?

## Regresión lineal simple
### Definición

![](./images/mod1/2_1.png)

## Regresión lineal simple

- Se considera el modelo 

$$Y = \beta_0 + \beta_1X + \varepsilon,$$
cuyos [coeficientes]{.bg4} (variables) son $\beta_0$ y $\beta_1$.

Se estiman los coeficientes para predecir las ventas

$$y = \hat{\beta_0} + \hat{\beta_1}x$$

## Regresión lineal simple
### Solución 

- Se define el [residuo de suma cuadrática]{.bg4} como 

$$RSS = e_1^2+\cdots+e_n^2,$$
con $e_i = y_i-\hat{y_i} = y_i-\hat{\beta_0}-\hat{\beta_1}x_i$

- Se puede resolver analíticamente:

$$\hat{\beta_1} = \dfrac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2} \text{ y } \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}.$$

## Regresión lineal simple
### Intervalo de confianza

- El [error estándar]{.bg4} de los estimadores es:

$$SE(\hat{\beta_1})^2 = \dfrac{\sigma^2}{\sum\limits_{i=1}^n(x_i-\bar{x})} \text{ y } SE(\hat{\beta_0})^2 = \sigma^2\left[\frac{1}{n} + \dfrac{\bar{x}^2}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}\right]$$

- [Intervalo de confianza]{.bg4} con $95\%$ de confianza 

$$\hat{\beta_i} \pm 2\cdot SE(\hat{\beta_i}).$$

## Regresión lineal simple
### Test de Hipótesis

- El [test de hipótesis]{.bg4} más común, involucra testear la [hipótesis nula]{.bg3} de:
  + $H_0:$ No hay relación entre $X$ e $Y$ ($\beta_1 = 0$).
  + $H_A:$ Hay relación entre $X$ e $Y$ ($\beta_1 \neq 0$).

- Para testearlo se usa el [estadístico $t$]{.bg4}

$$t = \dfrac{\hat{\beta_1}-0}{SE(\hat{\beta_1})} \sim t-\text{Student}(n-2)$$

- [p-value]{.bg3} es la probabilidad de observar un valor $\ge|t|$.

## Regresión lineal simple
### Ejemplo publicidad 

| | Coeficiente | SE | t-stadistic |p-value|
|:---------:|:-----:|:------:|:------:|:------:|
| Intercepto | 7.0325   | 0.4578 |   15.36 | < 0.0001 |
| TV | 0.0475   | 0.0027 | 17.67 | < 0.0001 |

- Para tener un p-value bajo $0.05$ se requiere un t-stadistic de, al menos, 2.
- La probabilidad de ver este resultado bajo la hipótesis nula es $<0.0001$.
- Se rechaza la hipótesis nula, i.e. $\beta_1\neq0$.

## Regresión lineal simple
### Precisión del modelo

- [Error estándar residual:]{.bg4} estimado de la desviación estándar de $\varepsilon$.

$$RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum\limits_{i=1}^n(y_i-\hat{y_i})^2}$$

- [$R^2$:]{.bg3} proporción de la variabilidad en $Y$ que puede ser explicada usando $X$.

$$R^2 = 1 - \dfrac{RSS}{TSS} \stackrel{!}{=} \hat{Cor}(X,Y) ,$$
con $TSS = \sum\limits_{i=1}^n(y_i-\bar{y})^2$.

## Regresión lineal simple
### Precisión del modelo: Ejemplo

| Cantidad | Valor|
|:--------:|:----:|
| RSE | 3.26|
|$R^2$  | 0.612|

- El $61\%$ de la variabilidad en `ventas` se explica por una regresión lineal con `TV`.



# Regresión lineal múltiple{background-color="#40666e"}

## Regresión lineal múltiple
### Definición

- El modelo es:

$$Y = \beta_0 + \beta_1X_1 + \cdots \beta_pX_p + \varepsilon.$$

- $\beta_j$ se interpreta como el cambio promedio en $Y$ por unidad de aumento en $X_j$, manteniendo los demás predictores fijos.
- En el ejemplo de publicidad:

$$sales = β_0 + β_1\cdot TV + β_2\cdot radio + β_3 \cdot newspaper + \varepsilon.$$

## Regresión lineal múltiple
### Ejemplo presupuesto

![](./images/mod2/fig1.png){fig-align="center"}

## Regresión lineal múltiple
### Interpretar los coeficientes

- El escenario ideal es cuando los predictores no están correlacionados.
  + Cada coeficiente puede ser estimado por separado.
  + Interpretaciones del tipo cambio por unidad, *mutatis mutandis* son posibles.
- Correlación entre predictores causa problemas:
  + Varianza de los coeficientes tiende a aumentar.
  + Más difícil de interpretar (cambia $X_j$ cambia todo).

## Regresión lineal múltiple
### Solución

- Se define el [residuo de suma cuadrática]{.bg4} como 

$$RSS = e_1^2+\cdots+e_n^2,$$
con $e_i =  y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1} - \cdots - \hat{\beta_p}x_{ip}$

- Se puede resolver analíticamente, pero computacionalmente se resuelve rápidamente.
- $\hat{\beta} = (X^\top{X})^{-1}X^\top{Y}$

## Regresión lineal múltiple
### Ejemplo de publicidad 

| | Coeficiente | SE | t-stadistic | p-value |
|:--|:--:|:--:|:--:|:--:|
|Intercepto|2.939|0.3119|9.42|<0.0001|
|TV| 0.046|0.0014|32.81|<0.0001|
|radio|0.189|0.0086|21.89|<0.0001|
|periódico|-0.001|0.0059|-0.18|0.8599|

| | Coeficiente | SE | t-stadistic |p-value|
|:---------:|:-----:|:------:|:------:|:------:|
| Intercepto | 7.0325   | 0.4578 |   15.36 | < 0.0001 |
| TV | 0.0475   | 0.0027 | 17.67 | < 0.0001 |

## Regresión lineal múltiple
### Ejemplo de publicidad 

| | Coeficiente | SE | t-stadistic | p-value |
|:--|:--:|:--:|:--:|:--:|
|Intercepto|2.939|0.3119|9.42|<0.0001|
|TV| 0.046|0.0014|32.81|<0.0001|
|radio|0.189|0.0086|21.89|<0.0001|
|periódico|-0.001|0.0059|-0.18|0.8599|

| | Coeficiente | SE | t-stadistic |p-value|
|:---------:|:-----:|:------:|:------:|:------:|
| Intercepto | 9.312   | 0.563 |   16.54 | < 0.0001 |
| radio | 0.203   | 0.020 | 9.92 | < 0.00115 |

## Regresión lineal múltiple
### Ejemplo de publicidad 

| | Coeficiente | SE | t-stadistic | p-value |
|:--|:--:|:--:|:--:|:--:|
|Intercepto|2.939|0.3119|9.42|<0.0001|
|TV| 0.046|0.0014|32.81|<0.0001|
|radio|0.189|0.0086|21.89|<0.0001|
|periódico|-0.001|0.0059|-0.18|0.8599|

| | Coeficiente | SE | t-stadistic |p-value|
|:---------:|:-----:|:------:|:------:|:------:|
| Intercepto | 12.351   | 0.621 |   19.88 | < 0.0001 |
| periódico | 0.055   | 0.017 | 3.30 | 0.00115 |

## Regresión lineal múltiple
### Ejemplo de publicidad 

| | Coeficiente | SE | t-stadistic | p-value |
|:--|:--:|:--:|:--:|:--:|
|Intercepto|2.939|0.3119|9.42|<0.0001|
|TV| 0.046|0.0014|32.81|<0.0001|
|radio|0.189|0.0086|21.89|<0.0001|
|periódico|-0.001|0.0059|-0.18|0.8599|

- Correlaciones

| | TV | radio | periódico | ventas|
|:--|:--:|:--:|:--:|:--:|
|TV|1.0000|0.0548|0.0567|0.7822|
|radio||1.0000|[0.3541]{.alert}|0.5762|
|periódico|||1.0000|0.2283|
|ventas||||1.000|

## Regresión lineal múltiple
### Preguntas
1. ¿Es alguno de los predictores $X_1,\ldots,X_p$ útil para predecir la respuesta?

- $H_0:\,\beta_1=\cdots=\beta_p = 0$
- $H_A:$ al menos uno de los $\beta_j\neq0$.
- Calcular Estadístico F:
  $$F = \dfrac{(TSS-RSS)/p}{RSS/(n-p-1)}$$
- Bajo $H_0$ se espera que $F\approx1$, si no, se espera $F\gg1$.


## Regresión lineal múltiple
### Preguntas: Ejemplo publicidad
1. ¿Es alguno de los predictores $X_1,\ldots,X_p$ útil para predecir la respuesta?

En la regresión multilineal

|Cantidad|Valor|
|:--|:--:|
|RSE|1.69|
|$R^2$|0.897|
|F-statistic|570|

- [¿Qué tan grande $F$?]{.bg3} Depende, si errores son normales, sigue distribución $F$ (p-value).

## Regresión lineal múltiple
### Preguntas
1. ¿Es alguno de los predictores $X_1,\ldots,X_p$ útil para predecir la respuesta?

- ¿Y si queremos testear que algunos coeficientes específicos sean 0?

- $H_0:\,\beta_{p-q+1} = \cdots = \beta_{p} = 0$,
- Si $RSS_0$ es el RSS para el modelo sin esas variables:

$$F = \dfrac{(RSS_0-RSS)/q}{RSS/(n-p-1)}$$

- ¿Por qué no revisar solo los estadísticos individuales?

## Regresión lineal múltiple
### Preguntas
2. ¿Todos los predictores ayudan a explicar $Y$, o solo un subconjunto es útil?

- `Selección *forward*:` 
  + Se comienza con un modelo sin predictores.
  + Se ajustan $p$ regresiones lineales simples y se agrega al modelo la con menor RSS.
  + Se repite hasta un criterio de parada.

## Regresión lineal múltiple
### Preguntas
2. ¿Todos los predictores ayudan a explicar $Y$, o solo un subconjunto es útil?

- `Selección *backward*:` 
  + Se comienza con un modelo con todas las variables.
  + Se remueve la de mayor p-value.
  + Se repite hasta un criterio de parada.
* No puede utilizarse si $p>n$.

## Regresión lineal múltiple
### Preguntas
2. ¿Todos los predictores ayudan a explicar $Y$, o solo un subconjunto es útil?

- `Selección mixta:` 
  + Se comienza con un modelo sin predictores.
  + Se agregan variables según la selección *forward*.
  + Si alguna variable pasa a tener un p-value sobre una tolerancia, se remueve.

## Regresión lineal múltiple
### Preguntas
3. ¿Qué tan bien se ajusta el modelo a los datos?

- [Error estándar residual:]{.bg4}

$$RSE = \sqrt{\dfrac{1}{n-p-1}RSS}$$

  + RSS siempre disminuye en los datos de entrenamiento si aumentan los predictores.

  + RSE podría aumentar ($p$ vs $RSS$).
- $R^2 = Cor(Y,\hat{Y})^2$, correlación  entre la respuesta y el modelo lineal ajustado.

## Regresión lineal múltiple
### Predictores cualitativos

- Si hay un predictor cualitativo de dos niveles (ej: sí o no) se crea la variable

$$x_i = \begin{cases}1, \,\text{si para $i$ es sí}\\0, \,\text{si no,}\end{cases}$$

- Esto se refleja en el modelo

$$y_i = \beta_0+\beta_1x_1+\varepsilon_i = 
\begin{cases}
\beta_0 + \beta_1 + \varepsilon_i,\text{ si para $i$ es sí},\\
\beta_0+\varepsilon_i,\text{ si no.}
\end{cases}$$

## Regresión lineal múltiple
### Predictores cualitativos

- Si hay un predictor cualitativo de más de dos niveles (ej: rojo, verde, azul) se crean dos variables

$$x_{i1} = \begin{cases}1, \,\text{si para $i$ es rojo}\\0, \,\text{si no,}\end{cases}$$

$$x_{i2} = \begin{cases}1, \,\text{si para $i$ es verde}\\0, \,\text{si no,}\end{cases}$$

- Esto se refleja en el modelo

$$y_i = \beta_0+\beta_1x_{i1} + \beta_2x_{i2}+\varepsilon_i = 
\begin{cases}
\beta_0 + \beta_1 + \varepsilon_i,\text{ si para $i$ es rojo},\\
\beta_0 + \beta_2 + \varepsilon_i,\text{ si para $i$ es verde},\\
\beta_0+\varepsilon_i,\text{ si para $i$ es azul.}
\end{cases}$$


# Regresión polinomial{background-color="#40666e"}

## Regresión polinomial
### Definición

- El modelo de regresión polinomial es:

$$y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2+\cdots+\beta_dx_i^d+\varepsilon_i$$

<center>
![](./images/mod2/fig2.png){width=50%}
</center>

- Modelo es lineal en $\beta$.
- Menor interpretabilidad de los coeficientes.