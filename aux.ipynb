{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Módulo 3\n",
        "subtitle: Técnicas de Clasificación\n",
        "format:\n",
        "  clean-revealjs:\n",
        "    self-contained: true\n",
        "    theme: slides.scss\n",
        "    touch: true\n",
        "    slide-level: 2\n",
        "author:\n",
        "  - name: Eloy Alvarado Narváez\n",
        "    orcid: 0000-0001-7522-2327\n",
        "    email: eloy.alvarado@usm.cl\n",
        "    affiliations: Universidad Técnica Federico Santa María\n",
        "  - name: Esteban Salgado Valenzuela\n",
        "    orcid: 0000-0002-7799-0044\n",
        "    affiliations: Universidad Técnica Federico Santa María\n",
        "date: 12/13/2024\n",
        "lang: es\n",
        "logo: images/logo_usm.png\n",
        "bibliography: refs.bib\n",
        "---\n",
        "\n",
        "\n",
        "# Técnicas de clasificación {background-color=\"#40666e\"}\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Como fue expuesto en el módulo anterior, los modelos de regresión lineal asumen que la variable respuesta $Y$ es [cuantitativa]{.bg3}. Sin embargo, en muchas situaciones prácticas, la variable en estudio es de tipo [cualitativa]{.bg1} (también referida como categórica).\n",
        "\n",
        "[Ejemplos:]{.bg4}\n",
        "\n",
        "1. Una persona llega a la sala de emergencias con un conjunto de síntomas que podrían atribuirse a una de tres condiciones médicas. [¿Cuál de las tres condiciones tiene el individuo?]{.bg2}\n",
        "\n",
        "2. Un servicio de banca en línea debe ser capaz de [determinar si una transacción realizada en el sitio es fraudulenta]{.bg2}, en función de la dirección IP del usuario, el historial de transacciones anteriores, entre otros factores.  \n",
        "\n",
        "## Introducción\n",
        "\n",
        "Al igual que en las regresiones, en el contexto de [clasificación]{.bg1} tenemos un conjunto de observaciones de entrenamiento:\n",
        "$$(x_1, y_1), \\dots, (x_n, y_n),$$\n",
        "\n",
        "que podemos usar para construir un [clasificador]{.bg3}. Queremos que nuestro clasificador tenga un [buen desempeño]{.bg4} no sólo en los datos de entrenamiento, sino también en observaciones de prueba que no se utilizaron para entrenar el clasificador.\n",
        "\n",
        "## Ejemplo: Mora en Tarjeta de Crédito\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![](./images/mod3/4_1a.png)\n",
        "\n",
        "![](./images/mod3/4_1b.png)\n",
        "\n",
        ":::\n",
        "\n",
        "## Ejemplo: Mora en Tarjeta de Crédito\n",
        "### ¿Podemos usar regresión lineal?\n"
      ],
      "id": "1b93f6ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from ISLP import load_data\n",
        "import pandas as pd\n",
        "Default = load_data('Default')\n",
        "Default.head(3)"
      ],
      "id": "d4cb7e05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supongamos que la clasificación de [mora]{.bg4} la codificamos como:\n",
        "\n",
        "$$Y= \\begin{cases} 0 \\quad \\text{si} \\quad \\texttt{default=No} \\\\\n",
        "1 \\quad \\text{si} \\quad \\texttt{default=Yes} \\end{cases}\n",
        "$$\n",
        "\n",
        "::: {.box3}\n",
        "¿Podríamos realizar una regresión lineal de $Y$ respecto de $X$ y clasificar como [$\\texttt{Yes}$]{.bg3} si $\\widehat{Y}>0.5$?\n",
        ":::\n",
        "\n",
        "## Ejemplo: Mora en Tarjeta de Crédito\n",
        "### ¿Podemos usar regresión lineal?\n",
        "\n",
        "- En el caso de una [respuesta binaria]{.bg1}, la regresión lineal es un [buen clasificador]{.bg3}, y es equivalente a realizar un [análisis discriminante lineal]{.bg4}\n",
        "\n",
        "- Debido a que en la población (de datos), $$\\mathbb{E}(Y| X = x)= \\mathbb{P}(Y=1 | X=x),$$\n",
        "se podría pensar que la regresión es un modelo ideal para este tipo de tareas.\n",
        "\n",
        "- Sin embargo, la [regresión lineal]{.bg2} podría producir probabilidades fuera del rango $[0,1]$. Así, la [regresión logística]{.bg4} es un modelo más apropiado.\n",
        "\n",
        "## Ejemplo: Mora en Tarjeta de Crédito\n",
        "### Regresión lineal vs Regresión logística\n",
        "<center>\n",
        "![](./images/mod3/4_2.png)\n",
        "</center>\n",
        "\n",
        "## Regresión logística\n",
        "\n",
        "Por simplicidad escribamos $p(X)=\\mathbb{P}(Y=1 | X)$ y consideremos la variable [`balance`]{.bg3} para predecir si hubo [mora (`default`)]{.bg4}. El modelo de regresión tiene por forma:\n",
        "\n",
        "$$p(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X)}{1+\\exp(\\beta_0 + \\beta_1 X)}$$\n",
        "\n",
        "Es claro ver, que independiente de los valores de $\\beta_0,\\beta_1$ o $X$, [$p(X)$ tomará valores en el intervalo $[0,1]$]{.bg1}. La expresión anterior la podemos reordenar como:\n",
        "\n",
        "$$\\ln\\left( \\dfrac{p(X)}{1-p(X)}\\right)=\\beta_0 + \\beta_1 X$$\n",
        "\n",
        "## Estimación de los coeficientes de regresión\n",
        "\n",
        "Los coeficiente $\\beta_0$ y $\\beta_1$ en la ecuación\n",
        "\n",
        "$$\n",
        "p(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X)}{1+\\exp(\\beta_0 + \\beta_1 X)}\n",
        "$$\n",
        "\n",
        "[son desconocidos]{.bg4}, por lo que [deben ser estimados basándose en los datos de entrenamiento]{.bg3}.\n",
        "\n",
        "Usualmente la metodología de [máxima verosimilitud]{.bg1} es preferida para el proceso de estimación, debido a que tiene buenas propiedades estadísticas.\n",
        "\n",
        "## Estimación de los coeficientes de regresión: continuación\n",
        "\n",
        "Formalmente, definimos la [función de verosimilitud]{.bg3} como:\n",
        "\n",
        "$$\n",
        "\\ell(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i':y_{i'}=0}(1-p(x_{i'}))\n",
        "$$\n",
        "\n",
        "Las estimaciones $\\hat{\\beta}_0$ y $\\hat{\\beta}_1$ son escogidos para [maximizar la función de verosimilitud]{.bg4}.\n",
        "\n",
        "## Regresión logística en `python`\n"
      ],
      "id": "8779ad12"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import statsmodels.api as sm\n",
        "Default['default'] = Default['default'].map({'No': 0, 'Yes': 1})\n",
        "X = Default[['balance']]\n",
        "X = sm.add_constant(X)  \n",
        "y = Default['default']\n",
        "model = sm.Logit(y, X)\n",
        "result = model.fit()"
      ],
      "id": "31dc9f79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión logística en `python`\n"
      ],
      "id": "956efcb8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "print(result.summary())"
      ],
      "id": "193755e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicciones\n",
        "\n",
        "¿Cuál es la probabilidad de [mora (`default`)]{.bg4} para alguien que tiene un [`balance`]{.bg3} de [$1000]{.fg1}?\n",
        "\n",
        "$$\n",
        "\\hat{p}(X)=\\dfrac{\\exp(-10.6513+ 0.0055 \\times 1000)}{1+\\exp(-10.6513+ 0.0055 \\times 1000)}\\approx 0.006\n",
        "$$\n",
        "\n",
        "¿Cuál es la probabilidad de [mora (`default`)]{.bg4} para alguien que tiene un [`balance`]{.bg3} de [$2000]{.fg1}?\n",
        "\n",
        "$$\n",
        "\\hat{p}(X)=\\dfrac{\\exp(-10.6513+ 0.0055 \\times 2000)}{1+\\exp(-10.6513+ 0.0055 \\times 2000)}\\approx 0.586\n",
        "$$\n",
        "\n",
        "## Mora de estudiantes\n",
        "\n",
        "### ¿Qué resultados obtenemos si el predictor es `student`?\n"
      ],
      "id": "c02841fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "Default['student'] = Default['student'].map({'No': 0, 'Yes': 1})\n",
        "X = Default[['student']]\n",
        "X = sm.add_constant(X)  \n",
        "y = Default['default']\n",
        "model = sm.Logit(y, X)\n",
        "result = model.fit()"
      ],
      "id": "a3a2cd78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mora de estudiantes{.small}\n"
      ],
      "id": "4c3b48b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "print(result.summary())"
      ],
      "id": "0411e74f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\mathbb{P}\\left( \\text{default=Yes }| \\text{ student=Yes}\\right)=\\dfrac{\\exp(-3.5041+ 0.4049 \\times 1)}{1+\\exp(-3.5041+ 0.4049 \\times 1)}\\approx 0.0431\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{P}\\left( \\text{default=Yes }| \\text{ student=No}\\right)=\\dfrac{\\exp(-3.5041+ 0.4049 \\times 0)}{1+\\exp(-3.5041+ 0.4049 \\times 0)}\\approx 0.0292\n",
        "$$\n",
        "\n",
        "## Regresión logística múltiple\n",
        "\n",
        "Ahora consideremos el problema de [predecir una respuesta binaria usando múltiples predictores]{.bg3}. La extensión natural del modelo de regresión es\n",
        "\n",
        "$$\n",
        "\\log \\left(\\dfrac{p(X)}{1-p(X)}\\right)=\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p\n",
        "$$\n",
        "\n",
        "donde $X=(X_1,\\dots,X_p)$ son $p$ predictores. La ecuación anterior la podemos reescribir como\n",
        "\n",
        "$$\n",
        "p(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}{1+ \\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}\n",
        "$$\n",
        "\n",
        "## Regresión logística múltiple\n"
      ],
      "id": "ba9a6a5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "X = Default[['student', 'balance', 'income']]\n",
        "X = sm.add_constant(X)\n",
        "y = Default['default']\n",
        "model = sm.Logit(y, X)\n",
        "result = model.fit()\n",
        "print(result.summary())"
      ],
      "id": "879c6778",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis discriminante {background-color=\"#40666e\"}\n",
        "## Introducción\n",
        "\n",
        "La regresión logística involucra [modelar directamente]{.bg4} $\\mathbb{P}\\left( Y=k|X=x\\right)$ usando la función logística dada por\n",
        "\n",
        "$$\n",
        "p(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}{1+ \\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}\n",
        "$$\n",
        "\n",
        "para el caso de [dos clases en la variable respuesta]{.bg3}. En lo que sigue, consideramos una manera alternativa y menos directa para estimar estas probabilidades.\n",
        "\n",
        "## Análisis discriminante lineal\n",
        "\n",
        "En esta metodología, [modelamos la distribución de los predictores]{.bg1} $X$ por separado en cada una de las categorías de la variable respuesta $(Y)$, y luego usamos el [teorema de Bayes]{.bg2} para convertir estos resultados en estimaciones de $\\mathbb{P}\\left(Y=k|X=x\\right)$.\n",
        "\n",
        "::: {.box3}\n",
        "[Teorema de Bayes]{.bg4}\n",
        "$$\\mathbb{P}(Y=k|X=x)=\\dfrac{\\mathbb{P}(X=x|Y=k)\\times \\mathbb{P}(Y=k)}{\\mathbb{P}(X=x)}\n",
        "$$\n",
        ":::\n",
        "\n",
        "## Análisis discriminante lineal\n",
        "\n",
        "Para las metodologías de análisis discriminante, reescribimos el [Teorema de Bayes]{.fg1} como:\n",
        "\n",
        "$$\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K}\\pi_l f_l(x)},$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $f_k(x)=\\mathbb{P}(X=x|Y=k)$ es la [densidad]{.bg3} para $X$ en la clase $k$.\n",
        "\n",
        "- $\\pi_k = \\mathbb{P}(Y=k)$ es la [probabilidad marginal o *apriori*]{.bg4} para la clase $k$.\n",
        "\n",
        "## Análisis discriminante lineal\n",
        "\n",
        "- La idea general, es [no estimar $p_k(X)$ directamente]{.bg3}, sino estimar $\\pi_k$ y $f_k$ para obtener lo deseado.\n",
        "\n",
        "- Usualmente [$\\pi_k$ es fácil de obtener]{.bg4} si se tiene una muestra aleatoria de $Y$, pues obtenemos estas estimaciones como las proporciones de cada clase.\n",
        "\n",
        "- Estimar [$f_k(X)$ tiende a ser más difícil]{.bg1}, a menos que se asuman formas simples para las densidades.\n",
        "\n",
        "## Análisis discriminante lineal con $p=1$\n",
        "\n",
        "Primero asumiremos que $p=1$, es decir, [sólo tenemos un predictor]{.bg2}. Deseamos obtener una estimación para $f_k(x)$ para utilizarlo en la ecuación\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n",
        "$$\n",
        "\n",
        "y así poder estimar $p_k(x)$. Para poder estimar $f_k$ [asumiremos que es *Gaussiana*]{.bg1}. Por lo que,\n",
        "\n",
        "$$\n",
        "f_k(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left( -\\dfrac{1}{2\\sigma_{k}^{2}}(x-\\mu_k)^2\\right)\n",
        "$$\n",
        "\n",
        "donde $\\mu_k$ y $\\sigma_{k}^{2}$ son la media y la varianza de la clase $k-$ésima. Por ahora, asumiremos que $\\sigma_{1}^{2}=\\dots=\\sigma_{K}^{2}=\\sigma^2$\n",
        "\n",
        "## Análisis discriminante lineal con $p=1$\n",
        "\n",
        "Por lo anterior, se tendrá\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(Y=k|X) = p_k(x)=\\dfrac{\\pi_k \\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( -\\dfrac{1}{2\\sigma^{2}}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^{K}\\pi_l\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( -\\dfrac{1}{2\\sigma^{2}}(x-\\mu_l)^2\\right) }\n",
        "$$\n",
        "\n",
        "::: {.box3}\n",
        "El clasificador Bayesiano [asigna una observacion $X=x$ a la clase que su $p_k(x)$ es más grande]{.bg3}. Si arreglamos términos en la expresión anterior, se tiene que el proceso es equivalente a asignar la observación a la clase en la que\n",
        "\n",
        "$$\n",
        "\\delta_k(x)=x \\dfrac{\\mu_k}{\\sigma^2}-\\dfrac{\\mu_{k}^{2}}{2\\sigma^2}+\\log \\pi_k\n",
        "$$\n",
        "\n",
        "es más grande.\n",
        ":::\n",
        "\n",
        "## Análisis discriminante lineal con $p=1$\n",
        "\n",
        "Por ejemplo, si $K=2$ Y $\\pi_1=\\pi_2$, entonces el [clasificador Bayesiano]{.bg3} asigna una observación a la clase 1 si: \n",
        "$$2x(\\mu_1-\\mu_2)>\\mu_{1}^{2}-\\mu_{2}^{2},$$\n",
        "\n",
        "y a la clase 2 en caso contrario. En este caso, el [límite de decisión de Bayes (*Bayes decision boundary*)]{.fg1} corresponde al punto donde\n",
        "\n",
        "$$\n",
        "x=\\dfrac{\\mu_{1}^{2}-\\mu_{2}^{2}}{2(\\mu_1-\\mu_2)}=\\dfrac{\\mu_1+\\mu_2}{2}\n",
        "$$\n",
        "\n",
        "Llamamos a este, el punto (o área) en donde [la clasificación es ambigua]{.bg2}.\n",
        "\n",
        "\n",
        "<center>\n",
        "![](./images/mod3/4_4.png)\n",
        "</center>\n",
        "\n",
        "## Análisis discriminante lineal con $p=1$\n",
        "\n",
        "El [análisis discriminante lineal (LDA)]{.bg1} aproxima el [clasificador bayesiano]{.bg4} ingresando las siguientes estimaciones:\n",
        "\n",
        "$$\n",
        "\\hat{\\pi}_k = \\dfrac{n_k}{n}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{\\mu}_k=\\dfrac{1}{n_k}\\sum_{i:y_i=k}x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^{2}=\\dfrac{1}{n-K}\\sum_{k=1}^{K}\\sum_{i:y_i=K}(x_i-\\hat{\\mu}_k)^2\n",
        "$$\n",
        "\n",
        "donde [$n$ es el número total de observaciones]{.bg2} en el conjunto de entrenamiento, [$n_k$ es el número de observaciones]{.bg3} en el conjunto de entrenamiento en la [clase $k-$ésima]{.bg3}.\n",
        "\n",
        "## Análisis discriminante lineal con $p>1$\n",
        "\n",
        "<center>\n",
        "![](./images/mod3/4_5.png)\n",
        "</center>\n",
        "\n",
        "- [Densidad:]{.bg3} $f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)}$\n",
        "\n",
        "- [Función discriminante:]{.bg4} $\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k$\n",
        "\n",
        "\n",
        "## Análisis discriminante lineal con $p>1$\n",
        "\n",
        "<center>\n",
        "![](./images/mod3/4_6.png)\n",
        "</center>\n",
        "\n",
        "## Otras formas de análisis discriminante\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n",
        "$$\n",
        "\n",
        "Cuando $f_k(x)$ son [distribuciones Gaussianas]{.bg1}, con la misma matriz de covarianza en cada clase, obtenemos el [Análisis Discriminante Lineal (LDA)]{.bg4}. Si alteramos la forma de $f_k(x)$, obtendremos diferentes clasificadores:\n",
        "\n",
        "- Con distribuciones Gaussianas pero diferentes $\\Sigma_k$ en cada clase, obtendremos el [Análisis Discriminante Cuadrático (QDA)]{.bg3}\n",
        "\n",
        "- Con $f_k(x)=\\prod_{j=1}^{p} f_{jk}(x_j)$ en cada clase obtendremos [Naive Bayes]{.bg3}\n",
        "\n",
        "- Muchas otras formas se pueden obtener al proponer modelos de densidad para $f_k(x)$.\n",
        "\n",
        "## Análisis discriminante cuadrático\n",
        "\n",
        "<center>\n",
        "![](./images/mod3/4_9.png)\n",
        "</center>\n",
        "\n",
        "$$\n",
        "\\delta_k(x) = -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n",
        "$$\n",
        "\n",
        "Debido a que $\\mathbf{\\Sigma_k}$ son diferentes, el término al cuadrado importa.\n",
        "\n",
        "## Naive Bayes\n",
        "\n",
        "- Asume que los predictores ($X$) son [independientes en cada clase]{.bg1}.\n",
        "\n",
        "- [Útil cuando $p$ es grande]{.bg3}, ya que los métodos multivariados como LDA o QDA son ineficientes.\n",
        "\n",
        "- Acepta predictores [cualitativos y cuantitativos]{.bg2}.\n",
        "\n",
        "- A pesar de tener muchos supuestos, [Naive Bayes usualmente entrega buenas clasificaciones]{.bg4}."
      ],
      "id": "66e8cdb7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
