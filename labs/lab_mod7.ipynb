{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afc399d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Módulo 7\n",
    "subtitle: Deep Learning\n",
    "author:\n",
    "  - name: Eloy Alvarado Narváez\n",
    "    orcid: 0000-0001-7522-2327\n",
    "    email: eloy.alvarado@usm.cl\n",
    "    affiliations: Universidad Técnica Federico Santa María\n",
    "  - name: Esteban Salgado Valenzuela\n",
    "    orcid: 0000-0002-7799-0044\n",
    "    affiliations: Universidad Técnica Federico Santa María\n",
    "date: 12/13/2024\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2304ae",
   "metadata": {},
   "source": [
    "Utilizamos el paquete `torch` de **Python**, junto con el paquete `pytorch_lightning`, que proporciona utilidades para simplificar el ajuste y la evaluación de modelos. El paquete está bien estructurado, es flexible y resultará cómodo para los usuarios familiarizados con **Python**.\n",
    "\n",
    "Un buen recurso complementario es el sitio [pytorch.org/tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html). \n",
    "\n",
    "Comenzamos cargando varias librerias como en los laboratorios pasados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bd6b71",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from sklearn.linear_model import \\\n",
    "     (LinearRegression,\n",
    "      LogisticRegression,\n",
    "      Lasso)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from sklearn.model_selection import \\\n",
    "     (train_test_split,\n",
    "      GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b799d88",
   "metadata": {},
   "source": [
    "### Librerías específicas de Torch\n",
    "\n",
    "Hay varias importaciones necesarias para `torch`. Primero, importamos la biblioteca principal y las herramientas esenciales utilizadas para especificar redes estructuradas secuencialmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3298bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import RMSprop\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c32fa5",
   "metadata": {},
   "source": [
    "Hay varios otros paquetes auxiliares para `torch`. Por ejemplo: \n",
    " \n",
    "- El paquete **`torchmetrics`** tiene utilidades para calcular diversas métricas y evaluar el rendimiento al ajustar un modelo.  \n",
    "- El paquete **`torchinfo`** proporciona un resumen útil de las capas de un modelo.  \n",
    "\n",
    "Usamos la función `read_image()` al cargar imágenes de prueba.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeae9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import (MeanAbsoluteError,\n",
    "                          R2Score)\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049a398",
   "metadata": {},
   "source": [
    "El paquete **`pytorch_lightning`** es una interfaz de nivel algo más alto para **`torch`** que simplifica la especificación y el ajuste de modelos, al reducir la cantidad de **código repetitivo** necesario (en comparación con usar `torch` por sí solo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a39702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b27f4d",
   "metadata": {},
   "source": [
    "Para reproducir resultados, usamos `seed_everything()`. También indicaremos a **`torch`** que utilice **algoritmos deterministas** siempre que sea posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11bf06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(0, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd719f8",
   "metadata": {},
   "source": [
    "Utilizaremos varios conjuntos de datos incluidos con **`torchvision`** para el laboratorio: una red preentrenada para clasificación de imágenes, así como algunas **transformaciones** utilizadas para el preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062cd869",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.datasets import MNIST, CIFAR100\n",
    "from torchvision.models import (resnet50,\n",
    "                                ResNet50_Weights)\n",
    "from torchvision.transforms import (Resize,\n",
    "                                    Normalize,\n",
    "                                    CenterCrop,\n",
    "                                    ToTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a507a",
   "metadata": {},
   "source": [
    "- **`SimpleDataModule`** y **`SimpleModule`** son versiones simplificadas de objetos utilizados en **`pytorch_lightning`**, el módulo de alto nivel para ajustar modelos de **`torch`**.  \n",
    "\n",
    "Aunque es posible realizar usos más avanzados, como computación en **unidades de procesamiento gráfico (GPUs)** y procesamiento paralelo de datos en este módulo, no nos enfocaremos mucho en estos temas en este laboratorio.  \n",
    "\n",
    "- **`ErrorTracker`** maneja colecciones de objetivos (*targets*) y predicciones en cada **mini-batch** durante las etapas de validación o prueba, permitiendo calcular la métrica en todo el conjunto de datos de validación o prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a05884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.torch import (SimpleDataModule,\n",
    "                        SimpleModule,\n",
    "                        ErrorTracker,\n",
    "                        rec_num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f41487",
   "metadata": {},
   "source": [
    "Los datos preprocesados de **`IMDb`** son provenientes de **`keras`**, un paquete independiente para ajustar modelos de aprendizaje profundo.  \n",
    "\n",
    "Esto nos ahorra una cantidad significativa de **preprocesamiento** y nos permite enfocarnos en la especificación y ajuste de los propios modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.torch.imdb import (load_lookup,\n",
    "                             load_tensor,\n",
    "                             load_sparse,\n",
    "                             load_sequential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86123617",
   "metadata": {},
   "source": [
    "Finalmente, introducimos algunas **importaciones auxiliares** que no están directamente relacionadas con **`torch`**.  \n",
    "\n",
    "- La función **`glob()`** del módulo **`glob`** se utiliza para encontrar todos los archivos que coinciden con caracteres **comodín** (*wildcards*). La usaremos en nuestro ejemplo aplicando el modelo **`ResNet50`** a algunas de nuestras propias imágenes.  \n",
    "- El módulo **`json`** se utilizará para cargar un archivo **JSON** que nos permitirá buscar las clases y **identificar las etiquetas** de las imágenes en el ejemplo de **`ResNet50`**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0100ff28",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f81cea",
   "metadata": {},
   "source": [
    "## Red de Capa Única en los Datos de Hitters  \n",
    "\n",
    "Comenzamos ajustando modelos utilizando los datos de **`Hitters`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef59d53a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  ...  PutOuts  Assists  Errors  Salary  NewLeague\n",
       "1    315    81      7    24   38  ...      632       43      10   475.0          N\n",
       "2    479   130     18    66   72  ...      880       82      14   480.0          A\n",
       "3    496   141     20    65   78  ...      200       11       3   500.0          N\n",
       "4    321    87     10    39   42  ...      805       40       4    91.5          N\n",
       "5    594   169      4    74   51  ...      282      421      25   750.0          A\n",
       "\n",
       "[5 rows x 20 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hitters = load_data('Hitters').dropna()\n",
    "n = Hitters.shape[0]\n",
    "Hitters.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302823e",
   "metadata": {},
   "source": [
    "Ajustaremos dos modelos lineales (**mínimos cuadrados** y **lasso**) y compararemos su rendimiento con el de una **red neuronal**. Para esta comparación utilizaremos el **error absoluto medio (MAE)** en un conjunto de datos de validación:\n",
    "\n",
    "$$\n",
    "\\text{MAE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|.\n",
    "$$\n",
    "\n",
    "Configuramos la matriz de características y la variable de respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4deacf91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = MS(Hitters.columns.drop('Salary'), intercept=False)\n",
    "X = model.fit_transform(Hitters).to_numpy()\n",
    "Y = Hitters['Salary'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030ccab",
   "metadata": {},
   "source": [
    "El método **`to_numpy()`** convierte **data frames** o **series** de `pandas` en arreglos de **`numpy`**.  \n",
    "Hacemos esto porque necesitaremos usar **`sklearn`** para ajustar el modelo **lasso**, y esta librería requiere dicha conversión.\n",
    "\n",
    "También utilizamos un método de **regresión lineal** de `sklearn`, en lugar del método de **`statsmodels`** para **facilitar las comparaciones**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffb53a",
   "metadata": {},
   "source": [
    "Ahora dividimos los datos en **entrenamiento** y **prueba**, fijando el **estado aleatorio** utilizado por `sklearn` para realizar la división.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8593f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, \n",
    " X_test,\n",
    " Y_train,\n",
    " Y_test) = train_test_split(X,\n",
    "                            Y,\n",
    "                            test_size=1/3,\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e980ad",
   "metadata": {},
   "source": [
    "### Modelos Lineales  \n",
    "Ajustamos el modelo lineal y evaluamos directamente el error en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b14993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259.71528833146294"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_lm = LinearRegression().fit(X_train, Y_train)\n",
    "Yhat_test = hit_lm.predict(X_test)\n",
    "np.abs(Yhat_test - Y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18abe13",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo **lasso** utilizando **`sklearn`**. Usamos el **error absoluto medio (MAE)** para seleccionar y evaluar el modelo, en lugar del **error cuadrático medio (MSE)**.  \n",
    "\n",
    "Aquí creamos una **malla de validación cruzada** y la ejecutamos directamente.\n",
    "\n",
    "Codificamos un **pipeline** con dos pasos:  \n",
    "1. Primero **normalizamos** las características utilizando un transformador **`StandardScaler()`**.  \n",
    "2. Luego ajustamos el modelo **lasso** sin realizar una normalización adicional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84c668ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "lasso = Lasso(warm_start=True, max_iter=30000)\n",
    "standard_lasso = Pipeline(steps=[('scaler', scaler),\n",
    "                                 ('lasso', lasso)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2c215",
   "metadata": {},
   "source": [
    "Necesitamos crear una **malla de valores** para $\\lambda$. Como es común, elegimos una malla de **100 valores** de $\\lambda$, distribuidos de manera uniforme en la **escala logarítmica** desde `lam_max` hasta `0.01 \\cdot \\text{lam_max}`.  \n",
    "\n",
    "Aquí, `lam_max` es el valor más pequeño de $\\lambda$ que produce una **solución completamente cero**. Este valor es igual al **mayor producto interno absoluto** entre cualquier predictor y la respuesta (centrada).  \n",
    "{La derivación de este resultado está fuera del alcance de este libro.}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093f8ce9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_s = scaler.fit_transform(X_train)\n",
    "n = X_s.shape[0]\n",
    "lam_max = np.fabs(X_s.T.dot(Y_train - Y_train.mean())).max() / n\n",
    "param_grid = {'lasso__alpha': np.exp(np.linspace(0, np.log(0.01), 100))\n",
    "             * lam_max}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def35611",
   "metadata": {},
   "source": [
    "Es importante notar que tuvimos que **transformar los datos** primero, ya que la escala de las variables impacta la elección de $\\lambda$.  \n",
    "\n",
    "Ahora realizamos **validación cruzada** utilizando esta secuencia de valores de $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38773381",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(10,\n",
    "           shuffle=True,\n",
    "           random_state=1)\n",
    "grid = GridSearchCV(standard_lasso,\n",
    "                    param_grid,\n",
    "                    cv=cv,\n",
    "                    scoring='neg_mean_absolute_error')\n",
    "grid.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2dc95",
   "metadata": {},
   "source": [
    "Extraemos el modelo **lasso** con el **menor error absoluto medio** validado cruzadamente y evaluamos su rendimiento en `X_test` y `Y_test`, que no fueron utilizados en la validación cruzada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc24a51",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.6754837478029"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_lasso = grid.best_estimator_\n",
    "Yhat_test = trained_lasso.predict(X_test)\n",
    "np.fabs(Yhat_test - Y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e9dc1",
   "metadata": {},
   "source": [
    "Esto es similar a los resultados obtenidos para el modelo lineal ajustado por **mínimos cuadrados**. Sin embargo, estos resultados pueden variar considerablemente con diferentes divisiones de entrenamiento/prueba.\n",
    "\n",
    "### Especificación de una Red: Clases y Herencia\n",
    "\n",
    "Para ajustar la **red neuronal**, primero configuramos una **estructura de modelo** que describe la red.  \n",
    "Esto requiere que definamos **nuevas clases** específicas para el modelo que deseamos ajustar.  \n",
    "\n",
    "Típicamente, esto se hace en **`pytorch`** sub-clasificando una representación genérica de una red, que es el enfoque que tomamos aquí.  \n",
    "\n",
    "Aunque este ejemplo es **simple**, repasaremos los pasos con cierto detalle, ya que esto nos será útil para el resto de ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "415cf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HittersModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(HittersModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(50, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return torch.flatten(self.sequential(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca326fd5",
   "metadata": {},
   "source": [
    "La declaración **`class`** identifica el bloque de código como una **declaración de clase** llamada `HittersModel`, que hereda de la clase base **`nn.Module`**. Esta clase base es **ampliamente utilizada** en **`torch`** y representa las **mapeos** en las redes neuronales.\n",
    "\n",
    "Indentados bajo la declaración de la clase están los **métodos** de esta clase:  \n",
    "en este caso, **`__init__`** y **`forward`**.  \n",
    "\n",
    "- El método **`__init__`** es llamado cuando se crea una instancia de la clase, como se muestra en la celda a continuación.  \n",
    "- En los métodos, **`self`** siempre hace referencia a una instancia de la clase.  \n",
    "\n",
    "En el método **`__init__`**, hemos adjuntado dos objetos a `self` como atributos:  \n",
    "- **`flatten`**  \n",
    "- **`sequential`**  \n",
    "\n",
    "Estos atributos se utilizan en el método **`forward`** para describir el **mapeo** que implementa este módulo.\n",
    "\n",
    "### Uso de `super()`\n",
    "\n",
    "Hay una línea adicional en el método **`__init__`**, que es una llamada a **`super()`**.  \n",
    "\n",
    "- Esta función permite a las **subclases** (es decir, `HittersModel`) acceder a los métodos de la clase que heredan.  \n",
    "- Por ejemplo, la clase **`nn.Module`** tiene su propio método `__init__`, que es diferente del método **`HittersModel.__init__()`** que hemos escrito.  \n",
    "\n",
    "Usar **`super()`** nos permite llamar al método de la clase base.  \n",
    "Para los modelos de **`torch`**, siempre haremos esta llamada a **`super()`** ya que es **necesaria** para que el modelo sea correctamente interpretado por **`torch`**.\n",
    "\n",
    "### Métodos adicionales de `nn.Module`\n",
    "\n",
    "El objeto **`nn.Module`** tiene más métodos además de **`__init__`** y **`forward`**. Estos métodos son **directamente accesibles** a instancias de `HittersModel` debido a la **herencia**.\n",
    "\n",
    "Uno de estos métodos, es el método **`eval()`**, que se utiliza para **desactivar dropout** cuando queremos evaluar el modelo en **datos de prueba**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60469c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_model = HittersModel(X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114db3d4",
   "metadata": {},
   "source": [
    "El objeto **`self.sequential`** es una composición de **cuatro mapeos**.  \n",
    "\n",
    "1. El primer mapeo lleva las **19 características** de **`Hitters`** a **50 dimensiones**, introduciendo $50 \\times 19 + 50$ parámetros para los pesos y el **intercepto** del mapeo (a menudo llamado *bias*).  \n",
    "2. Esta capa se mapea a una capa **ReLU**.  \n",
    "3. Luego es seguida por una capa de **dropout del 40%**.  \n",
    "4. Finalmente, se aplica un mapeo lineal a **1 dimensión**, nuevamente con un *bias*.  \n",
    "\n",
    "El número total de parámetros entrenables es:  \n",
    "\n",
    "$$\n",
    "50 \\times 19 + 50 + 50 + 1 = 1051.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5be00d",
   "metadata": {},
   "source": [
    "El paquete **`torchinfo`** proporciona la función **`summary()`** que resume esta información de manera ordenada.  \n",
    "\n",
    "Podemos especificar el **tamaño de la entrada** y ver el tamaño de cada **tensor** a medida que pasa por las capas de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84720ca7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "HittersModel                             [175, 19]                 [175]                     --\n",
       "├─Flatten: 1-1                           [175, 19]                 [175, 19]                 --\n",
       "├─Sequential: 1-2                        [175, 19]                 [175, 1]                  --\n",
       "│    └─Linear: 2-1                       [175, 19]                 [175, 50]                 1,000\n",
       "│    └─ReLU: 2-2                         [175, 50]                 [175, 50]                 --\n",
       "│    └─Dropout: 2-3                      [175, 50]                 [175, 50]                 --\n",
       "│    └─Linear: 2-4                       [175, 50]                 [175, 1]                  51\n",
       "===================================================================================================================\n",
       "Total params: 1,051\n",
       "Trainable params: 1,051\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.18\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.07\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(hit_model, \n",
    "        input_size=X_train.shape,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48833aad",
   "metadata": {},
   "source": [
    "Ahora necesitamos transformar nuestros datos de entrenamiento en una forma accesible para **`torch`**.  \n",
    "El tipo de dato básico en **`torch`** es un **`tensor`**.\n",
    "\n",
    "También notamos que **`torch`** típicamente trabaja con números de punto flotante de **32 bits** (*precisión simple*) en lugar de **64 bits** (*precisión doble*).  \n",
    "Por lo tanto, convertimos nuestros datos a **`np.float32`** antes de formar el tensor.\n",
    "\n",
    "Los tensores $X$ y $Y$ se organizan luego en un **`Dataset`** reconocido por **`torch`** utilizando **`TensorDataset()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f8038ce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train.astype(np.float32))\n",
    "Y_train_t = torch.tensor(Y_train.astype(np.float32))\n",
    "hit_train = TensorDataset(X_train_t, Y_train_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c854a",
   "metadata": {},
   "source": [
    "Hacemos lo mismo para el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffe8314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = torch.tensor(X_test.astype(np.float32))\n",
    "Y_test_t = torch.tensor(Y_test.astype(np.float32))\n",
    "hit_test = TensorDataset(X_test_t, Y_test_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806a619",
   "metadata": {},
   "source": [
    "Finalmente, este **dataset** se pasa a un **`DataLoader()`**, que en última instancia entrega los datos a nuestra red. Aunque esto puede parecer **excesivamente complejo**, esta estructura es útil para tareas más **avanzadas**, donde los datos pueden residir en diferentes **máquinas** o deben pasarse a una **GPU**.\n",
    "\n",
    "- Uno de sus argumentos es **`num_workers`**, que indica cuántos procesos utilizaremos para cargar los datos.  \n",
    "- Para datos pequeños como **`Hitters`**, esto tendrá poco efecto.  \n",
    "- Sin embargo, ofrece una **ventaja** para los ejemplos de **`MNIST`** y **`CIFAR100`** presentados más adelante.\n",
    "\n",
    "El paquete **`torch`** inspeccionará el proceso en ejecución y determinará el **número máximo de trabajadores**.  \n",
    "{Esto depende del **hardware de cómputo** y del número de **núcleos disponibles**.}\n",
    "\n",
    "Hemos incluido una función **`rec_num_workers()`** para calcular esto y saber cuántos trabajadores podrían ser razonables (en este caso, el máximo fue **16**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "337f4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_workers = rec_num_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18928238",
   "metadata": {},
   "source": [
    "La configuración general de **entrenamiento** en **`pytorch_lightning`** involucra datos de **entrenamiento**, **validación** y **prueba**.  \n",
    "Estos se representan mediante diferentes **data loaders**.  \n",
    "\n",
    "- Durante cada **epoch**, ejecutamos un **paso de entrenamiento** para aprender el modelo y un **paso de validación** para rastrear el error.  \n",
    "- Los datos de prueba se utilizan típicamente al final del entrenamiento para **evaluar el modelo**.\n",
    "\n",
    "En este caso, como sólo dividimos los datos en **entrenamiento** y **prueba**, utilizaremos los datos de **prueba** como datos de **validación** utilizando el argumento `validation=hit_test`.\n",
    "\n",
    "- El argumento **`validation`** puede ser:  \n",
    "   - Un **float** entre **0** y **1**: interpretado como un **porcentaje** de las observaciones de **entrenamiento** a usar para validación.  \n",
    "   - Un **entero**: interpretado como el **número** de observaciones de **entrenamiento** a usar para validación.  \n",
    "   - Un **`Dataset`**: se pasa directamente a un **data loader**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c60b1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_dm = SimpleDataModule(hit_train,\n",
    "                          hit_test,\n",
    "                          batch_size=32,\n",
    "                          num_workers=min(4, max_num_workers),\n",
    "                          validation=hit_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f527d",
   "metadata": {},
   "source": [
    "A continuación, debemos proporcionar un módulo de **`pytorch_lightning`** que controle los pasos realizados durante el proceso de entrenamiento.  \n",
    "\n",
    "Proporcionamos métodos para nuestro **`SimpleModule()`** que simplemente registran el valor de la **función de pérdida** y cualquier **métrica adicional** al final de cada época.  \n",
    "\n",
    "Estas operaciones son controladas por los métodos:  \n",
    "- `SimpleModule.training_step()`  \n",
    "- `SimpleModule.test_step()`  \n",
    "- `SimpleModule.validation_step()`  \n",
    "\n",
    "Sin embargo, **no modificaremos** estos métodos en nuestros ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "916cfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_module = SimpleModule.regression(hit_model,\n",
    "                           metrics={'mae':MeanAbsoluteError()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f2d1b",
   "metadata": {},
   "source": [
    "Al usar el método **`SimpleModule.regression()`**, indicamos que utilizaremos la **pérdida de error cuadrático**. También solicitamos que se registre el **error absoluto medio (MAE)** dentro de las métricas que se **loggean**.\n",
    "\n",
    "Registramos nuestros resultados mediante **`CSVLogger()`**, que en este caso almacena los resultados en un archivo **CSV** dentro del directorio **`logs/hitters`**.  \n",
    "\n",
    "Una vez que el ajuste está completo, esto nos permite cargar los resultados como un **`pd.DataFrame()`** y visualizarlos a continuación.  \n",
    "\n",
    "Existen varias maneras de registrar los resultados en **`pytorch_lightning`**, aunque no cubriremos esos detalles aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44a44fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_logger = CSVLogger('logs', name='hitters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f19f0",
   "metadata": {},
   "source": [
    "Finally we are ready to train our model and log the results. We\n",
    "use the `Trainer()` object from `pytorch_lightning`\n",
    "to do this work. The argument `datamodule=hit_dm` tells the trainer\n",
    "how training/validation/test logs are produced,\n",
    "while the first argument `hit_module`\n",
    "specifies the network architecture\n",
    "as well as the training/validation/test steps.\n",
    "The `callbacks` argument allows for\n",
    "several tasks to be carried out at various\n",
    "points while training a model. Here\n",
    "our `ErrorTracker()` callback will enable\n",
    "us to compute validation error while training\n",
    "and, finally, the test error.\n",
    "We now fit the model for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91122a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "hit_trainer = Trainer(deterministic=True,\n",
    "                      max_epochs=50,\n",
    "                      log_every_n_steps=5,\n",
    "                      logger=hit_logger,\n",
    "                      callbacks=[ErrorTracker()])\n",
    "hit_trainer.fit(hit_module, datamodule=hit_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e67f2d",
   "metadata": {},
   "source": [
    "At each step of SGD, the algorithm randomly selects 32 training observations for\n",
    "the computation of the gradient. Recall from Section~\\ref{Ch13:sec:fitt-neur-netw}\n",
    "that an epoch amounts to the number of SGD steps required to process $n$\n",
    "observations. Since the training set has\n",
    "$n=175$, and we specified a `batch_size` of 32 in the construction of  `hit_dm`, an epoch is $175/32=5.5$ SGD steps.\n",
    "\n",
    "After having fit the model, we can evaluate performance on our test\n",
    "data using the `test()` method of our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6012d1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hit_trainer.test(hit_module, datamodule=hit_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcc245",
   "metadata": {},
   "source": [
    "The results of the fit have been logged into a CSV file. We can find the\n",
    "results specific to this run in the `experiment.metrics_file_path`\n",
    "attribute of our logger. Note that each time the model is fit, the logger will output\n",
    "results into a new subdirectory of our directory `logs/hitters`.\n",
    "\n",
    "We now create a plot of the MAE (mean absolute error) as a function of\n",
    "the number of epochs.\n",
    "First we retrieve the logged summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48313d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_results = pd.read_csv(hit_logger.experiment.metrics_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be9d88",
   "metadata": {},
   "source": [
    "Since we will produce similar plots in later examples, we write a\n",
    "simple generic function to produce this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ccf9b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def summary_plot(results,\n",
    "                 ax,\n",
    "                 col='loss',\n",
    "                 valid_legend='Validation',\n",
    "                 training_legend='Training',\n",
    "                 ylabel='Loss',\n",
    "                 fontsize=20):\n",
    "    # Extraer datos manualmente para evitar errores con pandas.plot\n",
    "    epochs = results['epoch']\n",
    "    train_values = results[f'train_{col}_epoch']\n",
    "    valid_values = results[f'valid_{col}']\n",
    "    \n",
    "    # Graficar las curvas manualmente\n",
    "    ax.plot(epochs, train_values, marker='o', color='black', label=training_legend)\n",
    "    ax.plot(epochs, valid_values, marker='o', color='red', label=valid_legend)\n",
    "\n",
    "    # Etiquetas y leyenda\n",
    "    ax.set_xlabel('Epoch', fontsize=fontsize)\n",
    "    ax.set_ylabel(ylabel, fontsize=fontsize)\n",
    "    ax.legend(loc='best', fontsize=fontsize * 0.8)\n",
    "    \n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b080cf1",
   "metadata": {},
   "source": [
    "We now set up our axes, and use our function to produce the MAE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77370a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "ax = summary_plot(hit_results,\n",
    "                  ax,\n",
    "                  col='mae',\n",
    "                  ylabel='MAE',\n",
    "                  valid_legend='Validation (=Test)')\n",
    "\n",
    "ax.set_ylim([0, 400])\n",
    "ax.set_xticks(np.linspace(0, 50, 11).astype(int))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b2b37",
   "metadata": {},
   "source": [
    "We can predict directly from the final model, and\n",
    "evaluate its performance on the test data.\n",
    "Before fitting, we call the `eval()` method\n",
    "of `hit_model`.\n",
    "This tells\n",
    "`torch` to effectively consider this model to be fitted, so that\n",
    "we can use it to predict on new data. For our model here,\n",
    "the biggest change is that the dropout layers will\n",
    "be turned off, i.e. no weights will be randomly\n",
    "dropped in predicting on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a48733",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "hit_model.eval() \n",
    "preds = hit_module(X_test_t)\n",
    "torch.abs(Y_test_t - preds).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24663bc",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51594bb",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "In setting up our data module, we had initiated\n",
    "several worker processes that will remain running.\n",
    "We delete all references to the torch objects to ensure these processes\n",
    "will be killed.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca542d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del(Hitters,\n",
    "    hit_model, hit_dm,\n",
    "    hit_logger,\n",
    "    hit_test, hit_train,\n",
    "    X, Y,\n",
    "    X_test, X_train,\n",
    "    Y_test, Y_train,\n",
    "    X_test_t, Y_test_t,\n",
    "    hit_trainer, hit_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42547131",
   "metadata": {},
   "source": [
    "## Multilayer Network on the MNIST Digit Data\n",
    "The `torchvision` package comes with a number of example datasets,\n",
    "including the `MNIST`  digit data. Our first step is to retrieve\n",
    "the training and test data sets; the `MNIST()` function within\n",
    "`torchvision.datasets` is provided for this purpose. The\n",
    "data will be downloaded the first time this function is executed, and stored in the directory `data/MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train, \n",
    " mnist_test) = [MNIST(root='data',\n",
    "                      train=train,\n",
    "                      download=True,\n",
    "                      transform=ToTensor())\n",
    "                for train in [True, False]]\n",
    "mnist_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf4a88",
   "metadata": {},
   "source": [
    "There are 60,000 images in the training data and 10,000 in the test\n",
    "data. The images are $28\\times 28$, and stored as a matrix of pixels. We\n",
    "need to transform each one into a vector.\n",
    "\n",
    "Neural networks are somewhat sensitive to the scale of the inputs, much as ridge and\n",
    "lasso regularization are affected by scaling.  Here the inputs are eight-bit\n",
    "grayscale values between 0 and 255, so we rescale to the unit\n",
    "interval. {Note: eight bits means $2^8$, which equals 256. Since the convention\n",
    "is to start at $0$, the possible values  range from $0$ to $255$.}\n",
    "This transformation, along with some reordering\n",
    "of the axes, is performed by the `ToTensor()` transform\n",
    "from the `torchvision.transforms` package.\n",
    "\n",
    "As in our `Hitters` example, we form a data module\n",
    "from the training and test datasets, setting aside 20%\n",
    "of the training images for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dm = SimpleDataModule(mnist_train,\n",
    "                            mnist_test,\n",
    "                            validation=0.2,\n",
    "                            num_workers=max_num_workers,\n",
    "                            batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73614d27",
   "metadata": {},
   "source": [
    "Let’s take a look at the data that will get fed into our network. We loop through the first few\n",
    "chunks of the test dataset, breaking after 2 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf940c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for idx, (X_ ,Y_) in enumerate(mnist_dm.train_dataloader()):\n",
    "    print('X: ', X_.shape)\n",
    "    print('Y: ', Y_.shape)\n",
    "    if idx >= 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b2d96",
   "metadata": {},
   "source": [
    "We see that the $X$ for each batch consists of 256 images of size `1x28x28`.\n",
    "Here the `1` indicates a single channel (greyscale). For RGB images such as `CIFAR100` below,\n",
    "we will see that the `1` in the size will be replaced by `3` for the three RGB channels.\n",
    "\n",
    "Now we are ready to specify our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c49bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        self._forward = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            nn.Linear(128, 10))\n",
    "    def forward(self, x):\n",
    "        return self._forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25c568",
   "metadata": {},
   "source": [
    "We see that in the first layer, each `1x28x28` image is flattened, then mapped to\n",
    "256 dimensions where we apply a ReLU activation with 40% dropout.\n",
    "A second layer maps the first layer’s output down to\n",
    "128 dimensions, applying a ReLU activation with 30% dropout. Finally,\n",
    "the 128 dimensions are mapped down to 10, the number of classes in the\n",
    "`MNIST`  data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = MNISTModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e8c1b",
   "metadata": {},
   "source": [
    "We can check that the model produces output of expected size based\n",
    "on our existing batch `X_` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model(X_).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3158809",
   "metadata": {},
   "source": [
    "Let’s take a look at the summary of the model. Instead of an `input_size` we can pass\n",
    "a tensor of correct shape. In this case, we pass through the final\n",
    "batched `X_` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mnist_model,\n",
    "        input_data=X_,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a07df",
   "metadata": {},
   "source": [
    "Having set up both  the model and the data module, fitting this model is\n",
    "now almost identical to the `Hitters` example. In contrast to our regression model, here we will use the\n",
    "`SimpleModule.classification()` method which\n",
    "uses the  cross-entropy loss function instead of mean squared error. It must be supplied with the number of\n",
    "classes in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_module = SimpleModule.classification(mnist_model,\n",
    "                                           num_classes=10)\n",
    "mnist_logger = CSVLogger('logs', name='MNIST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c0f38",
   "metadata": {},
   "source": [
    "Now we are ready to go. The final step is to supply training data, and fit the model. We disable the progress bar below to avoid lengthy output in the browser when running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25515af7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mnist_trainer = Trainer(deterministic=True,\n",
    "                        max_epochs=30,\n",
    "                        logger=mnist_logger,\n",
    "                        enable_progress_bar=False,\n",
    "                        callbacks=[ErrorTracker()])\n",
    "mnist_trainer.fit(mnist_module,\n",
    "                  datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12d8c7",
   "metadata": {},
   "source": [
    "We have suppressed the output here, which is a progress report on the\n",
    "fitting of the model, grouped by epoch. This is very useful, since on\n",
    "large datasets fitting can take time. Fitting this model took 245\n",
    "seconds on a MacBook Pro with an Apple M1 Pro chip with 10 cores and 16 GB of RAM.\n",
    "Here we specified a\n",
    "validation split of 20%, so training is actually performed on\n",
    "80% of the 60,000 observations in the training set. This is an\n",
    "alternative to actually supplying validation data, like we did for the `Hitters` data.\n",
    "SGD  uses batches\n",
    "of 256 observations in computing the gradient, and doing the\n",
    "arithmetic, we see that an epoch corresponds to 188 gradient steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb25107e",
   "metadata": {},
   "source": [
    "`SimpleModule.classification()` includes\n",
    "an accuracy metric by default. Other\n",
    "classification metrics can be added from `torchmetrics`.\n",
    "We will use  our `summary_plot()` function to display \n",
    "accuracy across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fed35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mnist_results = pd.read_csv(mnist_logger.experiment.metrics_file_path)\n",
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "summary_plot(mnist_results,\n",
    "             ax,\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "ax.set_ylim([0.5, 1])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(np.linspace(0, 30, 7).astype(int));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c8d5c",
   "metadata": {},
   "source": [
    "Once again we evaluate the accuracy using the `test()` method of our trainer. This model achieves\n",
    "97% accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainer.test(mnist_module,\n",
    "                   datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c0bd0",
   "metadata": {},
   "source": [
    "Table~\\ref{Ch13:tab:mnist} also reports the error rates resulting from LDA (Chapter~\\ref{Ch4:classification}) and multiclass logistic\n",
    "regression. For LDA we refer the reader to Section~\\ref{Ch4-classification-lab:linear-discriminant-analysis}.\n",
    "Although we could use the `sklearn` function `LogisticRegression()` to fit  \n",
    "multiclass logistic regression, we are set up here to fit such a model\n",
    "with `torch`.\n",
    "We just have an input layer and an output layer, and omit the hidden layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_MLR, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(784, 10))\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "mlr_model = MNIST_MLR()\n",
    "mlr_module = SimpleModule.classification(mlr_model,\n",
    "                                         num_classes=10)\n",
    "mlr_logger = CSVLogger('logs', name='MNIST_MLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff23014",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mlr_trainer = Trainer(deterministic=True,\n",
    "                      max_epochs=30,\n",
    "                      enable_progress_bar=False,\n",
    "                      callbacks=[ErrorTracker()])\n",
    "mlr_trainer.fit(mlr_module, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8368ac1",
   "metadata": {},
   "source": [
    "We fit the model just as before and compute the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca159ef2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "mlr_trainer.test(mlr_module,\n",
    "                 datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be94ca6",
   "metadata": {},
   "source": [
    "The accuracy is above 90% even for this pretty simple model.\n",
    "\n",
    "As in the `Hitters` example, we delete some of\n",
    "the objects we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842c930",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del(mnist_test,\n",
    "    mnist_train,\n",
    "    mnist_model,\n",
    "    mnist_dm,\n",
    "    mnist_trainer,\n",
    "    mnist_module,\n",
    "    mnist_results,\n",
    "    mlr_model,\n",
    "    mlr_module,\n",
    "    mlr_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e98b5",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "In this section we fit a CNN to the `CIFAR100` data, which is available in the `torchvision`\n",
    "package. It is arranged in a similar fashion as the `MNIST` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb406355",
   "metadata": {},
   "outputs": [],
   "source": [
    "(cifar_train, \n",
    " cifar_test) = [CIFAR100(root=\"data\",\n",
    "                         train=train,\n",
    "                         download=True)\n",
    "             for train in [True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "cifar_train_X = torch.stack([transform(x) for x in\n",
    "                            cifar_train.data])\n",
    "cifar_test_X = torch.stack([transform(x) for x in\n",
    "                            cifar_test.data])\n",
    "cifar_train = TensorDataset(cifar_train_X,\n",
    "                            torch.tensor(cifar_train.targets))\n",
    "cifar_test = TensorDataset(cifar_test_X,\n",
    "                            torch.tensor(cifar_test.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4ea20",
   "metadata": {},
   "source": [
    "The `CIFAR100` dataset consists of 50,000 training images, each represented by a three-dimensional tensor:\n",
    "each three-color image is represented as a set of three channels, each of which consists of\n",
    "$32\\times 32$ eight-bit pixels. We standardize as we did for the\n",
    "digits, but keep the array structure. This is accomplished with the `ToTensor()` transform.\n",
    "\n",
    "Creating the data module is similar to the `MNIST`  example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980303a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cifar_dm = SimpleDataModule(cifar_train,\n",
    "                            cifar_test,\n",
    "                            validation=0.2,\n",
    "                            num_workers=max_num_workers,\n",
    "                            batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e007c",
   "metadata": {},
   "source": [
    "We again look at the shape of typical batches in our data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401888c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for idx, (X_ ,Y_) in enumerate(cifar_dm.train_dataloader()):\n",
    "    print('X: ', X_.shape)\n",
    "    print('Y: ', Y_.shape)\n",
    "    if idx >= 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82438c30",
   "metadata": {},
   "source": [
    "Before we start, we look at some of the training images; similar code produced\n",
    "Figure~\\ref{Ch13:fig:cifar100} on page \\pageref{Ch13:fig:cifar100}. The example below also illustrates\n",
    "that `TensorDataset` objects can be indexed with integers --- we are choosing\n",
    "random images from the training data by indexing `cifar_train`. In order to display correctly,\n",
    "we must reorder the dimensions by a call to `np.transpose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56090846",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = subplots(5, 5, figsize=(10,10))\n",
    "rng = np.random.default_rng(4)\n",
    "indices = rng.choice(np.arange(len(cifar_train)), 25,\n",
    "                     replace=False).reshape((5,5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        idx = indices[i,j]\n",
    "        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n",
    "                                      [1,2,0]),\n",
    "                                      interpolation=None)\n",
    "        axes[i,j].set_xticks([])\n",
    "        axes[i,j].set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333dca34",
   "metadata": {},
   "source": [
    "Here the `imshow()` method recognizes from the shape of its argument that it is a 3-dimensional array, with the last dimension indexing the three RGB color channels.\n",
    "\n",
    "We specify a moderately-sized  CNN for\n",
    "demonstration purposes, similar in structure to Figure~\\ref{Ch13:fig:DeepCNN}.\n",
    "We use several layers, each consisting of  convolution, ReLU, and max-pooling steps.\n",
    "We first define a module that defines one of these layers. As in our\n",
    "previous examples, we overwrite the `__init__()` and `forward()` methods\n",
    "of `nn.Module`. This user-defined  module can now be used in ways just like\n",
    "`nn.Linear()` or `nn.Dropout()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0567d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels):\n",
    "\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3,3),\n",
    "                              padding='same')\n",
    "        self.activation = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pool(self.activation(self.conv(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25956312",
   "metadata": {},
   "source": [
    "Notice that we used the `padding = \"same\"` argument to\n",
    "`nn.Conv2d()`, which ensures that the output channels have the\n",
    "same dimension as the input channels. There are 32 channels in the first\n",
    "hidden layer, in contrast to the three channels in the input layer. We\n",
    "use a $3\\times 3$ convolution filter for each channel in all the layers. Each\n",
    "convolution is followed by a max-pooling layer over $2\\times2$\n",
    "blocks.\n",
    "\n",
    "In forming our deep learning model for the `CIFAR100` data, we use several of our `BuildingBlock()`\n",
    "modules sequentially. This simple example\n",
    "illustrates some of the power of `torch`. Users can\n",
    "define modules of their own, which can be combined in other\n",
    "modules. Ultimately, everything is fit by a generic trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CIFARModel, self).__init__()\n",
    "        sizes = [(3,32),\n",
    "                 (32,64),\n",
    "                 (64,128),\n",
    "                 (128,256)]\n",
    "        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n",
    "                                    for in_, out_ in sizes])\n",
    "\n",
    "        self.output = nn.Sequential(nn.Dropout(0.5),\n",
    "                                    nn.Linear(2*2*256, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(512, 100))\n",
    "    def forward(self, x):\n",
    "        val = self.conv(x)\n",
    "        val = torch.flatten(val, start_dim=1)\n",
    "        return self.output(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf81883",
   "metadata": {},
   "source": [
    "We  build the model and look at the summary. (We had created examples of `X_` earlier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4e0a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cifar_model = CIFARModel()\n",
    "summary(cifar_model,\n",
    "        input_data=X_,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125ae4a",
   "metadata": {},
   "source": [
    "The total number of trainable parameters is 964,516.\n",
    "By studying the size of the parameters, we can see that the channels halve in both\n",
    "dimensions\n",
    "after each of these max-pooling operations. After the last of these we\n",
    "have a layer with  256 channels of dimension $2\\times 2$. These are then\n",
    "flattened to a dense layer of size 1,024;\n",
    "in other words, each of the $2\\times 2$ matrices is turned into a\n",
    "$4$-vector, and put side-by-side in one layer. This is followed by a\n",
    "dropout regularization layer,  then\n",
    "another dense layer of size 512, and finally, the\n",
    "output layer.\n",
    "\n",
    "Up to now, we have been using a default\n",
    "optimizer in `SimpleModule()`. For these data,\n",
    "experiments show that a smaller learning rate performs\n",
    "better than the default 0.01. We use a\n",
    "custom optimizer here with a learning rate of 0.001.\n",
    "Besides this, the logging and training\n",
    "follow a similar pattern to our previous examples. The optimizer\n",
    "takes an argument `params` that informs\n",
    "the optimizer which parameters are involved in SGD (stochastic gradient descent).\n",
    "\n",
    "We saw earlier that entries of a module’s parameters are tensors. In passing\n",
    "the parameters to the optimizer we are doing more than\n",
    "simply passing arrays; part of the structure of the graph\n",
    "is encoded in the tensors themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_optimizer = RMSprop(cifar_model.parameters(), lr=0.001)\n",
    "cifar_module = SimpleModule.classification(cifar_model,\n",
    "                                    num_classes=100,\n",
    "                                    optimizer=cifar_optimizer)\n",
    "cifar_logger = CSVLogger('logs', name='CIFAR100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_trainer = Trainer(deterministic=True,\n",
    "                        max_epochs=30,\n",
    "                        logger=cifar_logger,\n",
    "                        enable_progress_bar=False,\n",
    "                        callbacks=[ErrorTracker()])\n",
    "cifar_trainer.fit(cifar_module,\n",
    "                  datamodule=cifar_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6668f",
   "metadata": {},
   "source": [
    "This model can take 10 minutes or more to run and achieves about 42% accuracy on the test\n",
    "data. Although this is not terrible for 100-class data (a random\n",
    "classifier gets 1% accuracy), searching the web we see results around\n",
    "75%. Typically it takes a lot of architecture carpentry,\n",
    "fiddling with regularization, and time, to achieve such results.\n",
    "\n",
    "Let’s take a look at the validation and training accuracy\n",
    "across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf596b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "log_path = cifar_logger.experiment.metrics_file_path\n",
    "cifar_results = pd.read_csv(log_path)\n",
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "summary_plot(cifar_results,\n",
    "             ax,\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "ax.set_xticks(np.linspace(0, 10, 6).astype(int))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4a8bb",
   "metadata": {},
   "source": [
    "Finally, we evaluate our model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ad9bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cifar_trainer.test(cifar_module,\n",
    "                   datamodule=cifar_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52c614",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "As deep learning has become ubiquitous in machine learning, hardware\n",
    "manufacturers have produced special libraries that can\n",
    "often speed up the gradient-descent steps.\n",
    "\n",
    "For instance, Mac OS devices with the M1 chip may have the *Metal* programming framework\n",
    "enabled, which can speed up the  `torch` \n",
    "computations. We present an example of how to use this acceleration.\n",
    "\n",
    "The main changes are to the `Trainer()` call as well as to the metrics\n",
    "that will be evaluated on the data. These metrics must be told  where\n",
    "the data will be located at evaluation time. This is\n",
    "accomplished with a call to the `to()` method of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763b4c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for name, metric in cifar_module.metrics.items():\n",
    "        cifar_module.metrics[name] = metric.to('mps')\n",
    "    cifar_trainer_mps = Trainer(accelerator='mps',\n",
    "                                deterministic=True,\n",
    "                                enable_progress_bar=False,\n",
    "                                max_epochs=30)\n",
    "    cifar_trainer_mps.fit(cifar_module,\n",
    "                          datamodule=cifar_dm)\n",
    "    cifar_trainer_mps.test(cifar_module,\n",
    "                          datamodule=cifar_dm)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14791a8f",
   "metadata": {},
   "source": [
    "This yields approximately two- or three-fold  acceleration for each epoch.\n",
    "We have protected this code block using `try:` and `except:`\n",
    "clauses; if it works, we get the speedup, if it fails, nothing happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51532403",
   "metadata": {},
   "source": [
    "## Using Pretrained CNN Models\n",
    "We now show how to use a CNN pretrained on the  `imagenet` database to classify natural\n",
    "images, and demonstrate how we produced Figure~\\ref{Ch13:fig:homeimages}.\n",
    "We copied six JPEG images from a digital photo album into the\n",
    "directory `book_images`. These images are available\n",
    "from the data section of  <www.statlearning.com>, the ISLP book website. Download `book_images.zip`; when\n",
    "clicked it creates the `book_images` directory. \n",
    "\n",
    "The pretrained network we use is called `resnet50`; specification details can be found on the web.\n",
    "We will read in the images, and\n",
    "convert them into the array format expected by the `torch`\n",
    "software to match the specifications in `resnet50`. \n",
    "The conversion involves a resize, a crop and then a predefined standardization for each of the three channels.\n",
    "We now read in the images and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd3831",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "resize = Resize((232,232), antialias=True)\n",
    "crop = CenterCrop(224)\n",
    "normalize = Normalize([0.485,0.456,0.406],\n",
    "                      [0.229,0.224,0.225])\n",
    "imgfiles = sorted([f for f in glob('book_images/*')])\n",
    "imgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n",
    "                    for f in imgfiles])\n",
    "imgs = normalize(imgs)\n",
    "imgs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735ef73",
   "metadata": {},
   "source": [
    "We now set up the trained network with the weights we read in code block~6. The model has 50 layers, with a fair bit of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126ee25",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "resnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "summary(resnet_model,\n",
    "        input_data=imgs,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59077750",
   "metadata": {},
   "source": [
    "We set the mode to `eval()` to ensure that the model is ready to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53e23b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d63ea9",
   "metadata": {},
   "source": [
    "Inspecting the output above, we see that when setting up the\n",
    "`resnet_model`, the authors defined a `Bottleneck`, much like our\n",
    "`BuildingBlock` module.\n",
    "\n",
    "We now feed our six images through the fitted network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds = resnet_model(imgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8f653",
   "metadata": {},
   "source": [
    "Let’s look at the predicted probabilities for each of the top 3 choices. First we compute\n",
    "the probabilities by applying the softmax to the logits in `img_preds`. Note that\n",
    "we have had to call the `detach()` method on the tensor `img_preds` in order to convert\n",
    "it to our a more familiar `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_probs = np.exp(np.asarray(img_preds.detach()))\n",
    "img_probs /= img_probs.sum(1)[:,None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee6c41c",
   "metadata": {},
   "source": [
    "In order to see the class labels, we must download the index file associated with `imagenet`. {This is avalable from the book website and  [s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json](https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json).}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = json.load(open('imagenet_class_index.json'))\n",
    "class_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n",
    "                           labs.items()],\n",
    "                           columns=['idx', 'label'])\n",
    "class_labels = class_labels.set_index('idx')\n",
    "class_labels = class_labels.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d134d92",
   "metadata": {},
   "source": [
    "We’ll now construct a data frame for each image file\n",
    "with the labels with the three highest probabilities as\n",
    "estimated by the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369faef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i, imgfile in enumerate(imgfiles):\n",
    "    img_df = class_labels.copy()\n",
    "    img_df['prob'] = img_probs[i]\n",
    "    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n",
    "    print(f'Image: {imgfile}')\n",
    "    print(img_df.reset_index().drop(columns=['idx']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da904e",
   "metadata": {},
   "source": [
    "We see that the model\n",
    "is quite confident about `Flamingo.jpg`, but a little less so for the\n",
    "other images.\n",
    "\n",
    "We end this section with our usual cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585e5d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del(cifar_test,\n",
    "    cifar_train,\n",
    "    cifar_dm,\n",
    "    cifar_module,\n",
    "    cifar_logger,\n",
    "    cifar_optimizer,\n",
    "    cifar_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c2601",
   "metadata": {},
   "source": [
    "## IMDB Document Classification\n",
    "We now implement models for sentiment classification (Section~\\ref{Ch13:sec:docum-class})  on the `IMDB`\n",
    "dataset. As mentioned above code block~8, we are using\n",
    "a preprocessed version of the `IMDB` dataset found in the\n",
    "`keras` package. As `keras` uses `tensorflow`, a different\n",
    "tensor and deep learning library, we have\n",
    "converted the data to be suitable for `torch`. The\n",
    "code used to convert from `keras` is\n",
    "available in the module `ISLP.torch._make_imdb`. It\n",
    "requires some of the `keras` packages to run. These data use a dictionary of size 10,000.\n",
    "\n",
    "We have stored three different representations of the review data for this lab:\n",
    "\n",
    "* `load_tensor()`, a sparse tensor version usable by `torch`;\n",
    "* `load_sparse()`, a sparse matrix version usable by `sklearn`, since we will compare with a lasso fit;\n",
    "* `load_sequential()`, a padded\n",
    "version of the original sequence representation, limited to the last\n",
    "500 words of each review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30285383",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(imdb_seq_train,\n",
    " imdb_seq_test) = load_sequential(root='data/IMDB')\n",
    "padded_sample = np.asarray(imdb_seq_train.tensors[0][0])\n",
    "sample_review = padded_sample[padded_sample > 0][:12]\n",
    "sample_review[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf823309",
   "metadata": {},
   "source": [
    "The datasets `imdb_seq_train` and `imdb_seq_test` are\n",
    "both instances of the class `TensorDataset`. The\n",
    "tensors used to construct them can be found in the `tensors` attribute, with\n",
    "the first tensor  the features `X` and the second  the outcome `Y`.\n",
    "We have taken the first row of features and stored it as `padded_sample`. In the preprocessing\n",
    "used to form these data, sequences were padded with 0s in the beginning if they were\n",
    "not long enough, hence we remove this padding by restricting to entries where\n",
    "`padded_sample > 0`. We then provide the first 12 words of the sample review.\n",
    "\n",
    "We can find these words in the `lookup` dictionary from the `ISLP.torch.imdb` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = load_lookup(root='data/IMDB')\n",
    "' '.join(lookup[i] for i in sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8794d4",
   "metadata": {},
   "source": [
    "For our first model, we have created a binary feature for each\n",
    "of the 10,000 possible words in the dataset, with an entry of one\n",
    "in the $i,j$ entry if word $j$ appears in review $i$. As most reviews\n",
    "are quite short, such a feature matrix has over 98% zeros. These data\n",
    "are accessed using `load_tensor()` from the `ISLP` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4749f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "max_num_workers=10\n",
    "(imdb_train,\n",
    " imdb_test) = load_tensor(root='data/IMDB')\n",
    "imdb_dm = SimpleDataModule(imdb_train,\n",
    "                           imdb_test,\n",
    "                           validation=2000,\n",
    "                           num_workers=min(6, max_num_workers),\n",
    "                           batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1772177",
   "metadata": {},
   "source": [
    "We’ll use a two-layer model for our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400ab60",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class IMDBModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(IMDBModel, self).__init__()\n",
    "        self.dense1 = nn.Linear(input_size, 16)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = x\n",
    "        for _map in [self.dense1,\n",
    "                     self.activation,\n",
    "                     self.dense2,\n",
    "                     self.activation,\n",
    "                     self.output]:\n",
    "            val = _map(val)\n",
    "        return torch.flatten(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4596108",
   "metadata": {},
   "source": [
    "We now instantiate our model and look at a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51960f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\n",
    "summary(imdb_model,\n",
    "        input_size=imdb_test.tensors[0].size(),\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433c5a4",
   "metadata": {},
   "source": [
    "We’ll again use\n",
    "a smaller learning rate for these data,\n",
    "hence we pass an `optimizer` to the\n",
    "`SimpleModule`. \n",
    "Since the reviews are classified into\n",
    "positive or negative sentiment, we use\n",
    "`SimpleModule.binary_classification()`. {Our use of\n",
    "  `binary_classification()` instead of  `classification()` is\n",
    "  due to some subtlety in how `torchmetrics.Accuracy()` works,\n",
    "as well as the data type of the targets.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faa2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\n",
    "imdb_module = SimpleModule.binary_classification(\n",
    "                         imdb_model,\n",
    "                         optimizer=imdb_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fbb840",
   "metadata": {},
   "source": [
    "Having loaded the datasets into a data module\n",
    "and created a `SimpleModule`, the remaining steps\n",
    "are familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_logger = CSVLogger('logs', name='IMDB')\n",
    "imdb_trainer = Trainer(deterministic=True,\n",
    "                       max_epochs=30,\n",
    "                       logger=imdb_logger,\n",
    "                       enable_progress_bar=False,\n",
    "                       callbacks=[ErrorTracker()])\n",
    "imdb_trainer.fit(imdb_module,\n",
    "                 datamodule=imdb_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077efc8",
   "metadata": {},
   "source": [
    "Evaluating the test error yields roughly 86% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500cc62",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c7526",
   "metadata": {},
   "source": [
    "### Comparison to Lasso\n",
    "We now fit a lasso logistic regression model\n",
    " using `LogisticRegression()` from `sklearn`. Since `sklearn` does not recognize\n",
    "the sparse tensors of `torch`, we use a sparse\n",
    "matrix that is recognized by `sklearn.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e81d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "((X_train, Y_train),\n",
    " (X_valid, Y_valid),\n",
    " (X_test, Y_test)) = load_sparse(validation=2000,\n",
    "                                 random_state=0,\n",
    "                                 root='data/IMDB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8b857",
   "metadata": {},
   "source": [
    "Similar to what we did in\n",
    "Section~\\ref{Ch13-deeplearning-lab:single-layer-network-on-hitters-data},\n",
    "we construct a series of 50 values for the lasso reguralization parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3049398",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lam_max = np.abs(X_train.T * (Y_train - Y_train.mean())).max()\n",
    "lam_val = lam_max * np.exp(np.linspace(np.log(1),\n",
    "                                       np.log(1e-4), 50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08819f7",
   "metadata": {},
   "source": [
    "With `LogisticRegression()` the regularization parameter\n",
    "$C$ is specified as the inverse of $\\lambda$. There are several\n",
    "solvers for logistic regression; here we use `liblinear` which\n",
    "works well with the sparse input format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075f0b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(penalty='l1', \n",
    "                           C=1/lam_max,\n",
    "                           solver='liblinear',\n",
    "                           warm_start=True,\n",
    "                           fit_intercept=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8b4ff",
   "metadata": {},
   "source": [
    "The path of 50 values takes approximately 40 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "intercepts = []\n",
    "\n",
    "for l in lam_val:\n",
    "    logit.C = 1/l\n",
    "    logit.fit(X_train, Y_train)\n",
    "    coefs.append(logit.coef_.copy())\n",
    "    intercepts.append(logit.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33b90e",
   "metadata": {},
   "source": [
    "The coefficient and intercepts have an extraneous dimension which can be removed\n",
    "by the `np.squeeze()`  function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190268a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "coefs = np.squeeze(coefs)\n",
    "intercepts = np.squeeze(intercepts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271d2a2",
   "metadata": {},
   "source": [
    "We’ll now make a plot to compare our neural network results with the\n",
    "lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39221738",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, axes = subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "for ((X_, Y_),\n",
    "     data_,\n",
    "     color) in zip([(X_train, Y_train),\n",
    "                    (X_valid, Y_valid),\n",
    "                    (X_test, Y_test)],\n",
    "                    ['Training', 'Validation', 'Test'],\n",
    "                    ['black', 'red', 'blue']):\n",
    "    linpred_ = X_ * coefs.T + intercepts[None,:]\n",
    "    label_ = np.array(linpred_ > 0)\n",
    "    accuracy_ = np.array([np.mean(Y_ == l) for l in label_.T])\n",
    "    axes[0].plot(-np.log(lam_val / X_train.shape[0]),\n",
    "                 accuracy_,\n",
    "                 '.--',\n",
    "                 color=color,\n",
    "                 markersize=13,\n",
    "                 linewidth=2,\n",
    "                 label=data_)\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(r'$-\\log(\\lambda)$', fontsize=20)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e3d0e",
   "metadata": {},
   "source": [
    "Notice the use of `%%capture`, which suppresses the displaying of the partially completed figure. This is useful\n",
    "when making a complex figure, since the steps can be spread across two or more cells.\n",
    "We now add a plot of the lasso accuracy, and display the composed figure by simply entering its name at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba471b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "imdb_results = pd.read_csv(imdb_logger.experiment.metrics_file_path)\n",
    "summary_plot(imdb_results,\n",
    "             axes[1],\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "axes[1].set_xticks(np.linspace(0, 30, 7).astype(int))\n",
    "axes[1].set_ylabel('Accuracy', fontsize=20)\n",
    "axes[1].set_xlabel('Epoch', fontsize=20)\n",
    "axes[1].set_ylim([0.5, 1]);\n",
    "axes[1].axhline(test_results[0]['test_accuracy'],\n",
    "                color='blue',\n",
    "                linestyle='--',\n",
    "                linewidth=3)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2c89f",
   "metadata": {},
   "source": [
    "From the graphs we see that the accuracy of the lasso logistic regression peaks at about $0.88$,  as it does for  the neural network.\n",
    "\n",
    "Once again, we end with a cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c7df0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del(imdb_model,\n",
    "    imdb_trainer,\n",
    "    imdb_logger,\n",
    "    imdb_dm,\n",
    "    imdb_train,\n",
    "    imdb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9982b",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "In this lab we fit the models illustrated in\n",
    "Section~\\ref{Ch13:sec:recurr-neur-netw}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51642336",
   "metadata": {},
   "source": [
    "### Sequential Models for Document Classification\n",
    "Here we  fit a simple  LSTM RNN for sentiment prediction to\n",
    "the `IMDb` movie-review data, as discussed in Section~\\ref{Ch13:sec:sequ-models-docum}.\n",
    "For an RNN we use the sequence of words in a document, taking their\n",
    "order into account. We loaded the preprocessed \n",
    "data at the beginning of\n",
    "Section~\\ref{Ch13-deeplearning-lab:imdb-document-classification}.\n",
    "A script that details the preprocessing can be found in the\n",
    "`ISLP` library. Notably, since more than 90% of the documents\n",
    "had fewer than 500 words, we set the document length to 500. For\n",
    "longer documents, we used the last 500 words, and for shorter\n",
    "documents, we padded the front with blanks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_seq_dm = SimpleDataModule(imdb_seq_train,\n",
    "                               imdb_seq_test,\n",
    "                               validation=2000,\n",
    "                               batch_size=300,\n",
    "                               num_workers=min(6, max_num_workers)\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa82266",
   "metadata": {},
   "source": [
    "The first layer of the RNN is an embedding layer of size 32, which will be\n",
    "learned during  training. This layer one-hot encodes  each document\n",
    "as a matrix of dimension $500 \\times 10,003$, and then maps these\n",
    "$10,003$ dimensions down to $32$. {The extra 3 dimensions\n",
    "correspond to commonly occurring non-word entries in the reviews.}\n",
    " Since each word is represented by an\n",
    "integer, this is effectively achieved by the creation of an embedding\n",
    "matrix of size $10,003\\times 32$; each of the 500 integers in the\n",
    "document are then mapped to the appropriate 32 real numbers by\n",
    "indexing the appropriate rows of this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d5d9b",
   "metadata": {},
   "source": [
    "The second  layer is an LSTM with 32 units, and the output\n",
    "layer is a single logit for the binary classification task.\n",
    "In the last line of the `forward()` method below,\n",
    "we take the last 32-dimensional output of the LSTM and map it to our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cda21",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, 32)\n",
    "        self.lstm = nn.LSTM(input_size=32,\n",
    "                            hidden_size=32,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(32, 1)\n",
    "    def forward(self, x):\n",
    "        val, (h_n, c_n) = self.lstm(self.embedding(x))\n",
    "        return torch.flatten(self.dense(val[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0a983",
   "metadata": {},
   "source": [
    "We instantiate and take a look at the summary of the model, using the\n",
    "first 10 documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(X_test.shape[-1])\n",
    "summary(lstm_model,\n",
    "        input_data=imdb_seq_train.tensors[0][:10],\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9049a4",
   "metadata": {},
   "source": [
    "The 10,003 is suppressed in the summary, but we see it in the\n",
    "parameter count, since $10,003\\times 32=320,096$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_module = SimpleModule.binary_classification(lstm_model)\n",
    "lstm_logger = CSVLogger('logs', name='IMDB_LSTM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fc1fe",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lstm_trainer = Trainer(deterministic=True,\n",
    "                       max_epochs=20,\n",
    "                       logger=lstm_logger,\n",
    "                       enable_progress_bar=False,\n",
    "                       callbacks=[ErrorTracker()])\n",
    "lstm_trainer.fit(lstm_module,\n",
    "                 datamodule=imdb_seq_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6061b3",
   "metadata": {},
   "source": [
    "The rest is now similar to other networks we have fit. We\n",
    "track the test performance as the network is fit, and see that it attains 85% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer.test(lstm_module, datamodule=imdb_seq_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40823178",
   "metadata": {},
   "source": [
    "We once again show the learning progress, followed by cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774197a2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lstm_results = pd.read_csv(lstm_logger.experiment.metrics_file_path)\n",
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "summary_plot(lstm_results,\n",
    "             ax,\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "ax.set_xticks(np.linspace(0, 20, 5).astype(int))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0.5, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae5960",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "del(lstm_model,\n",
    "    lstm_trainer,\n",
    "    lstm_logger,\n",
    "    imdb_seq_dm,\n",
    "    imdb_seq_train,\n",
    "    imdb_seq_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a3c8f",
   "metadata": {},
   "source": [
    "### Time Series Prediction\n",
    "We now show how to fit the models in Section~\\ref{Ch13:sec:time-seri-pred}\n",
    "for  time series prediction.\n",
    "We first load and standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYSE = load_data('NYSE')\n",
    "cols = ['DJ_return', 'log_volume', 'log_volatility']\n",
    "X = pd.DataFrame(StandardScaler(\n",
    "                     with_mean=True,\n",
    "                     with_std=True).fit_transform(NYSE[cols]),\n",
    "                 columns=NYSE[cols].columns,\n",
    "                 index=NYSE.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5171b",
   "metadata": {},
   "source": [
    "Next we set up the lagged versions of the data, dropping\n",
    "any rows with missing values using the `dropna()`  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in range(1, 6):\n",
    "    for col in cols:\n",
    "        newcol = np.zeros(X.shape[0]) * np.nan\n",
    "        newcol[lag:] = X[col].values[:-lag]\n",
    "        X.insert(len(X.columns), \"{0}_{1}\".format(col, lag), newcol)\n",
    "X.insert(len(X.columns), 'train', NYSE['train'])\n",
    "X = X.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a02e25",
   "metadata": {},
   "source": [
    "Finally, we extract the response, training indicator, and drop the current day’s `DJ_return` and\n",
    "`log_volatility` to predict only from previous day’s data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a4c0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Y, train = X['log_volume'], X['train']\n",
    "X = X.drop(columns=['train'] + cols)\n",
    "X.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc66b8e",
   "metadata": {},
   "source": [
    "We first fit a simple linear model and compute the $R^2$ on the test data using\n",
    "the `score()`  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec80811",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = LinearRegression()\n",
    "M.fit(X[train], Y[train])\n",
    "M.score(X[~train], Y[~train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053ae12",
   "metadata": {},
   "source": [
    "We refit this model, including the factor variable `day_of_week`.\n",
    "For a categorical series in `pandas`, we can form the indicators\n",
    "using the `get_dummies()`  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2de6ca",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_day = pd.concat([X, \n",
    "                  pd.get_dummies(NYSE['day_of_week'])],\n",
    "                  axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b70b4",
   "metadata": {},
   "source": [
    " Note that we do not have\n",
    "to reinstantiate the linear regression model\n",
    "as its `fit()` method accepts a design matrix and a response directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff00b65",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M.fit(X_day[train], Y[train])\n",
    "M.score(X_day[~train], Y[~train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88374112",
   "metadata": {},
   "source": [
    "This model achieves an $R^2$ of about 46%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2160545",
   "metadata": {},
   "source": [
    "To fit the RNN, we must reshape the data, as it will expect 5 lagged\n",
    "versions of each feature as indicated by the  `input_shape` argument\n",
    "to the layer `nn.RNN()`  below. We first\n",
    "ensure the columns of  our data frame are such that a reshaped\n",
    "matrix will have the variables correctly lagged. We use the\n",
    "`reindex()`  method to do this.\n",
    "\n",
    "For an input shape `(5,3)`, each row represents a lagged version of the three variables.\n",
    "The `nn.RNN()` layer also expects the first row of each\n",
    "observation to be earliest in time, so we must reverse the current order.  Hence we loop over\n",
    "`range(5,0,-1)` below, which is\n",
    "an example of using a  `slice()`  to index\n",
    "iterable objects. The general notation is `start:end:step`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08107439",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ordered_cols = []\n",
    "for lag in range(5,0,-1):\n",
    "    for col in cols:\n",
    "        ordered_cols.append('{0}_{1}'.format(col, lag))\n",
    "X = X.reindex(columns=ordered_cols)\n",
    "X.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f38634",
   "metadata": {},
   "source": [
    "We now reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776f3d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_rnn = X.to_numpy().reshape((-1,5,3))\n",
    "X_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f3387",
   "metadata": {},
   "source": [
    "By specifying the first size as -1, `numpy.reshape()` deduces its size based on the remaining arguments.\n",
    "\n",
    "Now we are ready to proceed with the RNN, which uses 12 hidden units, and  10%\n",
    "dropout.\n",
    "After passing through the RNN,  we extract the final time point as `val[:,-1]`\n",
    "in `forward()` below. This gets passed through a 10% dropout and then flattened through\n",
    "a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYSEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NYSEModel, self).__init__()\n",
    "        self.rnn = nn.RNN(3,\n",
    "                          12,\n",
    "                          batch_first=True)\n",
    "        self.dense = nn.Linear(12, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        val, h_n = self.rnn(x)\n",
    "        val = self.dense(self.dropout(val[:,-1]))\n",
    "        return torch.flatten(val)\n",
    "nyse_model = NYSEModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abd764",
   "metadata": {},
   "source": [
    "We  fit the model in a similar fashion to previous networks. We\n",
    "supply the `fit` function with test data as validation data, so that when\n",
    "we monitor its progress and plot the history function we can see the\n",
    "progress on the test data. Of course we should not use this as a basis for\n",
    "early stopping, since then the test performance would be biased.\n",
    "\n",
    "We form the training dataset similar to\n",
    "our `Hitters` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for mask in [train, ~train]:\n",
    "    X_rnn_t = torch.tensor(X_rnn[mask].astype(np.float32))\n",
    "    Y_t = torch.tensor(Y[mask].astype(np.float32))\n",
    "    datasets.append(TensorDataset(X_rnn_t, Y_t))\n",
    "nyse_train, nyse_test = datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c1f1e",
   "metadata": {},
   "source": [
    "Following our usual pattern, we inspect the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10263b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "summary(nyse_model,\n",
    "        input_data=X_rnn_t,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba24d5c",
   "metadata": {},
   "source": [
    "We again put the two datasets into a data module, with a\n",
    "batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36032ef",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "nyse_dm = SimpleDataModule(nyse_train,\n",
    "                           nyse_test,\n",
    "                           num_workers=min(4, max_num_workers),\n",
    "                           validation=nyse_test,\n",
    "                           batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a7cf8",
   "metadata": {},
   "source": [
    "We run some data through our model to be sure the sizes match up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (x, y) in enumerate(nyse_dm.train_dataloader()):\n",
    "    out = nyse_model(x)\n",
    "    print(y.size(), out.size())\n",
    "    if idx >= 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85106c8",
   "metadata": {},
   "source": [
    "We follow our previous example for setting up a trainer for a\n",
    "regression problem, requesting the $R^2$ metric\n",
    "to be be computed at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_optimizer = RMSprop(nyse_model.parameters(),\n",
    "                         lr=0.001)\n",
    "nyse_module = SimpleModule.regression(nyse_model,\n",
    "                                      optimizer=nyse_optimizer,\n",
    "                                      metrics={'r2':R2Score()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a430a",
   "metadata": {},
   "source": [
    "Fitting the model should by now be familiar.\n",
    "The results on the test data are very similar to the linear AR model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff00af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nyse_trainer = Trainer(deterministic=True,\n",
    "                       max_epochs=200,\n",
    "                       enable_progress_bar=False,\n",
    "                       callbacks=[ErrorTracker()])\n",
    "nyse_trainer.fit(nyse_module,\n",
    "                 datamodule=nyse_dm)\n",
    "nyse_trainer.test(nyse_module,\n",
    "                  datamodule=nyse_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fb76e",
   "metadata": {},
   "source": [
    "We could also fit a model without the `nn.RNN()` layer by just\n",
    "using a `nn.Flatten()` layer instead. This would be a nonlinear AR model. If in addition we excluded the \n",
    "hidden layer, this would be equivalent to our earlier linear AR model.  \n",
    "\n",
    "Instead we will fit a nonlinear AR model using the feature set `X_day` that includes the `day_of_week` indicators.\n",
    "To do so, we\n",
    "must first create our test and training datasets and a corresponding\n",
    "data module. This may seem a little burdensome, but is part of the\n",
    "general pipeline for `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for mask in [train, ~train]:\n",
    "    X_day_t = torch.tensor(\n",
    "                   np.asarray(X_day[mask]).astype(np.float32))\n",
    "    Y_t = torch.tensor(np.asarray(Y[mask]).astype(np.float32))\n",
    "    datasets.append(TensorDataset(X_day_t, Y_t))\n",
    "day_train, day_test = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea59a7",
   "metadata": {},
   "source": [
    "Creating a data module follows a familiar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c70d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dm = SimpleDataModule(day_train,\n",
    "                          day_test,\n",
    "                          num_workers=min(4, max_num_workers),\n",
    "                          validation=day_test,\n",
    "                          batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a76bb",
   "metadata": {},
   "source": [
    "We build a `NonLinearARModel()` that takes as input the 20 features and a hidden layer  with 32 units. The remaining steps are familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fca594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearARModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonLinearARModel, self).__init__()\n",
    "        self._forward = nn.Sequential(nn.Flatten(),\n",
    "                                      nn.Linear(20, 32),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(0.5),\n",
    "                                      nn.Linear(32, 1))\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(self._forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a134064",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_model = NonLinearARModel()\n",
    "nl_optimizer = RMSprop(nl_model.parameters(),\n",
    "                           lr=0.001)\n",
    "nl_module = SimpleModule.regression(nl_model,\n",
    "                                        optimizer=nl_optimizer,\n",
    "                                        metrics={'r2':R2Score()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1a2df",
   "metadata": {},
   "source": [
    "We continue with the usual training steps, fit the model,\n",
    "and evaluate the test error. We see the test $R^2$ is a slight improvement over the linear AR model that also includes `day_of_week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591bf1d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "nl_trainer = Trainer(deterministic=True,\n",
    "                     max_epochs=20,\n",
    "                     enable_progress_bar=False,\n",
    "                     callbacks=[ErrorTracker()])\n",
    "nl_trainer.fit(nl_module, datamodule=day_dm)\n",
    "nl_trainer.test(nl_module, datamodule=day_dm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8c38e",
   "metadata": {},
   "source": [
    "              \n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
